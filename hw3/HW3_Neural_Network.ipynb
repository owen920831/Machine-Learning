{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IagZMs0_qjdL"
      },
      "source": [
        "# 1. Introduction\n",
        "\n",
        "Welcome to your third assignment. In this assignment, you will build a deep neural network step by step. In this notebook, you will implement all the functions required to build a neural network.\n",
        "\n",
        "After finishing this assignment, you will have a deeper understanding of the process of training a deep neural network, which only consists of three steps: forward propagation, backward propagation and update."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGFR00CQvoaH"
      },
      "source": [
        "# 2. Packages\n",
        "All the packages that you need to finish this assignment are listed below.\n",
        "*   numpy : the fundamental package for scientific computing with Python.\n",
        "*   matplotlib : a comprehensive library for creating static, animated, and interactive visualizations in Python.\n",
        "*   math : Python has a built-in module that you can use for mathematical tasks.\n",
        "*   sklearn.datasets : scikit-learn comes with a few small standard datasets that do not require to download any file from some external website. You will be using the breast cancer wisconsin dataset to build a binary classifier.\n",
        "\n",
        "⚠️ **WARNING** ⚠️: \n",
        "*   Please do not import any other packages.\n",
        "*   np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. Please don't change the seed.\n",
        "\n",
        "❗ **Important** ❗: Please do not change the code outside this code bracket.\n",
        "```\n",
        "### START CODE HERE ### (≈ n lines of code)\n",
        "...\n",
        "### END CODE HERE ###\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fmTH9UkeqdYf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn import datasets\n",
        "\n",
        "output = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w35ZkTwMc00G"
      },
      "source": [
        "# 3. Neural network\n",
        "In this section, you will need to implement a deep neural network from scratch all by yourself. If you are familiar with deep learning library, such as Tensorflow or PyTorch, it may seems easy for you. But if you don't, don't worry because we will guide you step by step. All you need to do is to follow the instructions and understand how each part works.\n",
        "\n",
        "As mentioned before, the process of training a deep neural network is composed of three steps: forward propagation, backward propagation, and update, so all the to-do in this section will be related to these three steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "x0KHo8w9yqbY"
      },
      "outputs": [],
      "source": [
        "class Dense():\n",
        "    def __init__(self, n_x, n_y, seed=1):\n",
        "        self.n_x = n_x\n",
        "        self.n_y = n_y\n",
        "        self.seed = seed\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"\n",
        "        Argument:\n",
        "        self.n_x -- size of the input layer\n",
        "        self.n_y -- size of the output layer\n",
        "        self.parameters -- python dictionary containing your parameters:\n",
        "                           W -- weight matrix of shape (n_y, n_x)\n",
        "                           b -- bias vector of shape (n_y, 1)\n",
        "        \"\"\"\n",
        "        np.random.seed(self.seed)\n",
        "\n",
        "        # GRADED FUNCTION: linear_initialize_parameters\n",
        "        ### START CODE HERE ### (≈ 6 lines of code)\n",
        "        limit = math.sqrt(6/(self.n_x+self.n_y))\n",
        "        W = np.random.uniform(-limit, limit, (self.n_y, self.n_x))\n",
        "        b = np.zeros((self.n_y, 1))\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        assert(W.shape == (self.n_y, self.n_x))\n",
        "        assert(b.shape == (self.n_y, 1))\n",
        "\n",
        "        self.parameters = {\"W\": W, \"b\": b}\n",
        "\n",
        "    def forward(self, A):\n",
        "        \"\"\"\n",
        "        Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "        Arguments:\n",
        "        A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "        self.cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "\n",
        "        Returns:\n",
        "        Z -- the input of the activation function, also called pre-activation parameter \n",
        "        \"\"\"\n",
        "\n",
        "        # GRADED FUNCTION: linear_forward\n",
        "        ### START CODE HERE ### (≈ 2 line of code)\n",
        "        self.cache = (A, self.parameters[\"W\"], self.parameters[\"b\"])\n",
        "        Z = np.dot(self.parameters[\"W\"], A) + self.parameters[\"b\"]\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        assert(Z.shape == (self.parameters[\"W\"].shape[0], A.shape[1]))\n",
        "        \n",
        "        return Z\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        \"\"\"\n",
        "        Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "        Arguments:\n",
        "        dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "        self.cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "        self.dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "        self.db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "\n",
        "        Returns:\n",
        "        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "\n",
        "        \"\"\"\n",
        "        A_prev, W, b = self.cache\n",
        "        m = A_prev.shape[1]\n",
        "\n",
        "        # GRADED FUNCTION: linear_backward\n",
        "        ### START CODE HERE ### (≈ 3 lines of code)\n",
        "        self.dW = 1/m*(np.dot(dZ, A_prev.T))\n",
        "        self.db = 1/m*(np.sum(dZ, axis=1)).reshape(b.shape)\n",
        "        #print(self.db.shape)\n",
        "        dA_prev = np.dot(W.T, dZ)\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        assert (dA_prev.shape == A_prev.shape)\n",
        "        assert (self.dW.shape == self.parameters[\"W\"].shape)\n",
        "        assert (self.db.shape == self.parameters[\"b\"].shape)\n",
        "        \n",
        "        return dA_prev\n",
        "\n",
        "    def update(self, learning_rate):\n",
        "        \"\"\"\n",
        "        Update parameters using gradient descent\n",
        "        \n",
        "        Arguments:\n",
        "        learning rate -- step size\n",
        "        \"\"\"\n",
        "\n",
        "        # GRADED FUNCTION: linear_update_parameters\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        self.parameters[\"W\"] = self.parameters[\"W\"] - learning_rate*self.dW\n",
        "        self.parameters[\"b\"] = self.parameters[\"b\"] - learning_rate*self.db\n",
        "        ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_krGKUNg_Ix"
      },
      "source": [
        "## 3.1 Implement a linear layer\n",
        "First, we will start by implementing one of the most commonly used layers in the deep neural network, called the dense layer. The dense layer is a linear layer applying a linear transformation to the incoming data:\n",
        "$Z = WA + b$, where $W$ and $b$ are the weight and bias.\n",
        "\n",
        "**Note**: Dense layers, also known as Fully-connected layers, connect every input neuron to every output neuron and are commonly used in neural networks.\n",
        "\n",
        "### 3.1.1. Initialize parameters\n",
        "**Exercise**: Create and initialize parameters of a linear layer using Glorot uniform initialization. (5%)\n",
        "\n",
        "**Instructions**:\n",
        "*   Use random initialization (uniform distribution) for the weight matrices. Draws samples from a uniform distribution within [-limit, limit], where limit = sqrt(6 / (fan_in + fan_out)) (fan_in is the number of input units in the weight tensor and fan_out is the number of output units).\n",
        "*   Use zero initialization for the biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7HNAWwmg8R7T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W = [[-0.20325375  0.53968259 -1.22446471]]\n",
            "b = [[0.]]\n"
          ]
        }
      ],
      "source": [
        "dense = Dense(3, 1)\n",
        "print(\"W = \" + str(dense.parameters[\"W\"]))\n",
        "print(\"b = \" + str(dense.parameters[\"b\"]))\n",
        "\n",
        "dense = Dense(4, 1)\n",
        "output[\"linear_initialize_parameters\"] = dense.parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtPtH0j3BFN7"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>W: </td>\n",
        "    <td>[[-0.20325375  0.53968259 -1.22446471]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b: </td>\n",
        "    <td>[[0.]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abu7YqxeAeMz"
      },
      "source": [
        "### 3.1.2. Linear forward\n",
        "\n",
        "After initializing parameters, you will need to apply the linear transformation to the incoming data, and this can be simply done by matrix multiplication and addition.\n",
        "\n",
        "**Exercise**: Implement linear forward by applying the linear transformation. (5%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SSf8JIyjaj_A"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Z = [[1.9 2.2 2.5]]\n"
          ]
        }
      ],
      "source": [
        "A, W, b = np.array([[0, 0.5, 1], [1, 1.5, 2], [2, 2.5, 3]]), np.array([[0.1, 0.2, 0.3]]), np.array([[1.1]])\n",
        "dense = Dense(3, 1)\n",
        "dense.parameters = {\"W\": W, \"b\": b}\n",
        "Z = dense.forward(A)\n",
        "print(\"Z = \" + str(Z))\n",
        "\n",
        "A, W, b = np.array([[0, -0.5, -1], [1, 1.5, 2], [-2, -2.5, -3]]), np.array([[0.5, 0.3, 0.7]]), np.array([[-1.1]])\n",
        "dense = Dense(3, 1)\n",
        "dense.parameters = {\"W\": W, \"b\": b}\n",
        "Z = dense.forward(A)\n",
        "output[\"linear_forward\"] = (Z, dense.cache)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpcPlE8-EUsR"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Z: </td>\n",
        "    <td>[[1.9 2.2 2.5]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K8_obj6vIeT"
      },
      "source": [
        "## 3.1.3. Linear backward\n",
        "Backpropagation is used to calculate the gradient of the loss function with respect to the parameters.\n",
        "\n",
        "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
        "\n",
        "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n",
        "\n",
        "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.Here are the formulas you need:\n",
        "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
        "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)} $$\n",
        "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$\n",
        "\n",
        "**Exercise**: Use the 3 formulas above to implement `linear_backward()`. (5%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fg-PfP31NKH7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dA_prev = [[3.5 6. ]]\n",
            "dW = [[1.625]\n",
            " [0.625]]\n",
            "db = [[2.  ]\n",
            " [0.75]]\n"
          ]
        }
      ],
      "source": [
        "dZ, linear_cache = np.array([[1.5, 2.5], [0.5, 1.0]]), (np.array([[0.5, 1]]), np.array([[2.0], [1.0]]), np.array([[0.5], [1.0]]))\n",
        "dense = Dense(1, 2)\n",
        "dense.cache = linear_cache\n",
        "\n",
        "dA_prev = dense.backward(dZ)\n",
        "print (\"dA_prev = \" + str(dA_prev))\n",
        "print (\"dW = \" + str(dense.dW))\n",
        "print (\"db = \" + str(dense.db))\n",
        "\n",
        "dZ, linear_cache = np.array([[0.5, -1.5], [-1.5, 2.0]]), (np.array([[0.25, 1.25]]), np.array([[-1.0], [1.0]]), np.array([[-0.5], [-1.0]]))\n",
        "dense = Dense(1, 2)\n",
        "dense.cache = linear_cache\n",
        "dA_prev = dense.backward(dZ)\n",
        "output[\"linear_backward\"] = (dA_prev, dense.dW, dense.db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny0k-zxuNKIB"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>dA_prev: </td>\n",
        "    <td>[[3.5 6. ]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW: </td>\n",
        "    <td>[[1.625]\n",
        " [0.625]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db: </td>\n",
        "    <td>[[2.  ]\n",
        " [0.75]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWNWxxutN47B"
      },
      "source": [
        "## 3.1.4. Linear update parameters\n",
        "In this section you will update the parameters of the linear layer, using gradient descent:\n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$\n",
        "\n",
        "**Exercise**: Implement update() to update your parameters using gradient descent. (5%)\n",
        "\n",
        "**Instructions**: \n",
        "*   Update parameters using gradient descent on $W^{[l]}$ and $b^{[l]}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BMBqHniLN47I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W = [[0.5]\n",
            " [2.5]]\n",
            "b = [[-1.]\n",
            " [ 2.]]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1)\n",
        "dense = Dense(1, 2)\n",
        "dense.parameters = {\"W\": np.array([[1.0], [2.0]]), \"b\": np.array([[0.5], [0.5]])}\n",
        "dense.dW = np.array([[0.5], [-0.5]])\n",
        "dense.db = np.array([[1.5], [-1.5]])\n",
        "dense.update(1.0)\n",
        "print(\"W = \" + str(dense.parameters[\"W\"]))\n",
        "print(\"b = \" + str(dense.parameters[\"b\"]))\n",
        "\n",
        "dense = Dense(3, 4)\n",
        "np.random.seed(1)\n",
        "parameters, grads = {\"W1\": np.random.rand(3, 4), \"b1\": np.random.rand(3,1), \"W2\": np.random.rand(1,3), \"b2\": np.random.rand(1,1)}, {\"dW1\": np.random.rand(3, 4), \"db1\": np.random.rand(3,1), \"dW2\": np.random.rand(1,3), \"db2\": np.random.rand(1,1)}\n",
        "dense.parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
        "dense.dW = grads[\"dW1\"]\n",
        "dense.db = grads[\"db1\"]\n",
        "dense.update(0.1)\n",
        "output[\"linear_update_parameters\"] = {\"W\": dense.parameters[\"W\"], \"b\": dense.parameters[\"b\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIl13uvgN47I"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>W1: </td>\n",
        "    <td>[[0.5]\n",
        " [2.5]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b1: </td>\n",
        "    <td>[[-1.]\n",
        " [ 2.]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syt1bV3bdI_f"
      },
      "source": [
        "## 3.2. Activation function layer\n",
        "\n",
        "In this section, you will need to implement activation function layers. There are many activation functions, such as sigmoid function, softmax function, ReLU function and etc. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Nnuv8MmebMgg"
      },
      "outputs": [],
      "source": [
        "class Activation():\n",
        "    def __init__(self, function):\n",
        "        self.function = function\n",
        "\n",
        "    def forward(self, Z):\n",
        "        if self.function == \"sigmoid\":\n",
        "            \"\"\"\n",
        "            Implements the sigmoid activation in numpy\n",
        "            \n",
        "            Arguments:\n",
        "            Z -- numpy array of any shape\n",
        "            self.cache -- stores Z as well, useful during backpropagation\n",
        "            \n",
        "            Returns:\n",
        "            A -- output of sigmoid(z), same shape as Z\n",
        "            \n",
        "            \"\"\"\n",
        "\n",
        "            # GRADED FUNCTION: sigmoid_forward\n",
        "            ### START CODE HERE ### (≈ 8 lines of code)\n",
        "            A = []\n",
        "            for row in Z:\n",
        "                vector = []\n",
        "                for col in row:\n",
        "                    if (col >= 0): vector = np.append(vector, 1/(1+np.exp(-col)))\n",
        "                    else : vector = np.append(vector, np.exp(col)/(1+np.exp(col)))\n",
        "                A = np.append(A, vector)\n",
        "            A = A.reshape(Z.shape)\n",
        "            self.cache = Z\n",
        "            \n",
        "            ### END CODE HERE ###\n",
        "            \n",
        "            return A\n",
        "\n",
        "        elif self.function == \"softmax\":\n",
        "            \"\"\"\n",
        "            Implements the softmax activation in numpy\n",
        "            \n",
        "            Arguments:\n",
        "            Z -- numpy array of any shape (dim 0: number of classes, dim 1: number of samples)\n",
        "            self.cache -- stores Z as well, useful during backpropagation\n",
        "            \n",
        "            Returns:\n",
        "            A -- output of softmax(z), same shape as Z\n",
        "            \"\"\"\n",
        "\n",
        "            # GRADED FUNCTION: softmax_forward\n",
        "            ### START CODE HERE ### (≈ 2 lines of code)\n",
        "            A = (np.exp(Z-np.max(Z)))/(np.sum(np.exp(Z-np.max(Z)), axis=0))\n",
        "            self.cache = Z\n",
        "            ### END CODE HERE ###\n",
        "            \n",
        "            return A\n",
        "\n",
        "        elif self.function == \"relu\":\n",
        "            \"\"\"\n",
        "            Implement the RELU function in numpy\n",
        "            Arguments:\n",
        "            Z -- numpy array of any shape\n",
        "            self.cache -- stores Z as well, useful during backpropagation\n",
        "            Returns:\n",
        "            A -- output of relu(z), same shape as Z\n",
        "            \n",
        "            \"\"\"\n",
        "            \n",
        "            # GRADED FUNCTION: relu_forward\n",
        "            ### START CODE HERE ### (≈ 2 lines of code)\n",
        "            A = np.where(Z>0, Z, 0)\n",
        "            self.cache = Z\n",
        "            ### END CODE HERE ###\n",
        "            \n",
        "            assert(A.shape == Z.shape)\n",
        "            \n",
        "            return A\n",
        "\n",
        "    def backward(self, dA=None, Y=None):\n",
        "        if self.function == \"sigmoid\":\n",
        "            \"\"\"\n",
        "            Implement the backward propagation for a single SIGMOID unit.\n",
        "            Arguments:\n",
        "            dA -- post-activation gradient, of any shape\n",
        "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "            Returns:\n",
        "            dZ -- Gradient of the cost with respect to Z\n",
        "            \"\"\"\n",
        "            \n",
        "            # GRADED FUNCTION: sigmoid_backward\n",
        "            ### START CODE HERE ### (≈ 9 lines of code)\n",
        "            Z = self.cache\n",
        "            A = []\n",
        "            for row in Z:\n",
        "                vector = []\n",
        "                for col in row:\n",
        "                    if (col >= 0): vector = np.append(vector, 1/(1+np.exp(-col)))\n",
        "                    else : vector = np.append(vector, np.exp(col)/(1+np.exp(col)))\n",
        "                A = np.append(A, vector)\n",
        "            A = A.reshape(Z.shape)\n",
        "\n",
        "            dZ = dA * (A*(1-A))\n",
        "            ### END CODE HERE ###\n",
        "            \n",
        "            assert (dZ.shape == Z.shape)\n",
        "            \n",
        "            return dZ\n",
        "\n",
        "        elif self.function == \"relu\":\n",
        "            \"\"\"\n",
        "            Implement the backward propagation for a single RELU unit.\n",
        "            Arguments:\n",
        "            dA -- post-activation gradient, of any shape\n",
        "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "            Returns:\n",
        "            dZ -- Gradient of the cost with respect to Z\n",
        "            \"\"\"\n",
        "            \n",
        "            # GRADED FUNCTION: relu_backward\n",
        "            ### START CODE HERE ### (≈ 3 lines of code)\n",
        "\n",
        "            Z = self.cache > 0\n",
        "            dZ = dA * Z # just converting dz to a correct object. \n",
        "            dZ[Z <= 0] = 0 # When z <= 0, you should set dz to 0 as well.\n",
        "            ### END CODE HERE ###\n",
        "            \n",
        "            assert (dZ.shape == Z.shape)\n",
        "            \n",
        "            return dZ\n",
        "\n",
        "        elif self.function == \"softmax\":\n",
        "            \"\"\"\n",
        "            Implement the backward propagation for a [SOFTMAX->CCE LOSS] unit.\n",
        "            Arguments:\n",
        "            Y -- true \"label\" vector (one hot vector, for example: [[1], [0], [0]] represents rock, [[0], [1], [0]] represents paper, [[0], [0], [1]] represents scissors \n",
        "                                      in a Rock-Paper-Scissors image classification), shape (number of classes, number of examples)\n",
        "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "            Returns:\n",
        "            dZ -- Gradient of the cost with respect to Z\n",
        "            \"\"\"\n",
        "            \n",
        "            # GRADED FUNCTION: softmax_CCE_backward\n",
        "            ### START CODE HERE ### (≈ 3 lines of code)\n",
        "            Z = self.cache\n",
        "            s = (np.exp(Z-np.max(Z)))/(np.sum(np.exp(Z-np.max(Z)), axis=0))\n",
        "            dZ = s-Y\n",
        "            ### END CODE HERE ###\n",
        "            \n",
        "            assert (dZ.shape == Z.shape)\n",
        "            \n",
        "            return dZ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PkLKaFWiWmF"
      },
      "source": [
        "### 3.2.1. Activation forward\n",
        "#### 3.2.1.1. Sigmoid function\n",
        "Sigmoid: $\\sigma(Z) = \\begin{cases}\n",
        "    \\frac{1}{1+e^{-Z}},& \\text{if } Z >= 0\\\\\n",
        "    \\frac{e^{Z}}{1+e^{Z}}, & \\text{otherwise}\n",
        "\\end{cases}$. \n",
        "\n",
        "❗**Important**❗: As you can see, there is an exponential function inside the sigmoid function, so you might encounter an exponential overflow problem when implementing this function. To solve this problem, we use the numerically stable sigmoid function as shown in the equation above.\n",
        "\n",
        "### 3.2.1.2. Softmax function\n",
        "Softmax: $\\sigma(\\vec{Z})_i = \\frac{e^{Z_i-b}}{\\sum_{j=1}^{K} e^{Z_j-b}}$, where $\\vec{Z}$ = input vector, $K$ = number of classes in the multi-class classifier, $b$ is $\\max_{j=1}^{K} Z_j$\n",
        "\n",
        "❗**Important**❗: The naive implementation $\\sigma(\\vec{Z})_i = \\frac{e^{Z_i}}{\\sum_{j=1}^{K} e^{Z_j}}$ is terrible when there are large numbers! You might encounter the following problems if you use the naive implementation.\n",
        "*   RuntimeWarning: overflow encountered in exp\n",
        "\n",
        "\n",
        "### 3.2.1.3. ReLU (rectified linear unit) function\n",
        "ReLU: $RELU(Z) = max(Z, 0)$\n",
        "\n",
        "**Exercise**: Implement activation function. (5%+5%) (basic: Sigmoid and ReLU, advanced: Softmax)\n",
        "\n",
        "**Instruction**: \n",
        "*   Sigmoid: This function returns one item and stores one item: the activation value \"a\" and a cache contains \"z\" (it's what we will use in to the corresponding backward function).\n",
        "*   Softmax: This function returns one item and stores one item: the activation value \"a\" and a cache contains \"z\" (it's what we will use in to the corresponding backward function).\n",
        "*   ReLU: This function returns one item and stores one item: the activation value \"a\" and a cache contains \"z\" (it's what we will use in to the corresponding backward function)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gBuRAoeUC5jV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sigmoid: A = [[0.00669285 0.26894142 0.5        0.73105858 0.99330715]]\n",
            "ReLU: A = [[0 0 0 1 5]]\n",
            "Softmax: A = \n",
            "[[0.0320586  0.1748777  0.0320586 ]\n",
            " [0.08714432 0.47536689 0.08714432]\n",
            " [0.23688282 0.1748777  0.23688282]\n",
            " [0.64391426 0.1748777  0.64391426]]\n"
          ]
        }
      ],
      "source": [
        "Z = np.array([[-5, -1, 0, 1, 5]])\n",
        "\n",
        "sigmoid = Activation(\"sigmoid\")\n",
        "A = sigmoid.forward(Z)\n",
        "print(\"Sigmoid: A = \" + str(A))\n",
        "A = sigmoid.forward(np.array([[-1.82, -0.71, 0.02, 0.13, 2.21]]))\n",
        "output[\"sigmoid\"] = (A, sigmoid.cache)\n",
        "\n",
        "relu = Activation(\"relu\")\n",
        "A = relu.forward(Z)\n",
        "print(\"ReLU: A = \" + str(A))\n",
        "A = relu.forward(np.array([[-1.82, -0.71, 0.02, 0.13, 2.21]]))\n",
        "output[\"relu\"] = (A, relu.cache)\n",
        "\n",
        "Z = np.array([[1, 0, -2], [2, 1, -1], [3, 0, 0], [4, 0, 1]])\n",
        "softmax = Activation(\"softmax\")\n",
        "A = softmax.forward(Z)\n",
        "print(\"Softmax: A = \\n\" + str(A))\n",
        "A = softmax.forward(np.array([[0.1, 1.2, -2.1], [2.2, 0.7, -1.3], [1.4, 0.3, 0.2], [3.9, 0.5, -1.6]]))\n",
        "output[\"softmax\"] = (A, softmax.cache) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyyX_xxdEmNp"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>(With sigmoid) A: </td>\n",
        "    <td>[[0.00669285 0.26894142 0.5        0.73105858 0.99330715]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(With ReLU) A: </td>\n",
        "    <td>[[0 0 0 1 5]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(With softmax) A: </td>\n",
        "    <td>[[0.0320586  0.1748777  0.0320586 ]\n",
        " [0.08714432 0.47536689 0.08714432]\n",
        " [0.23688282 0.1748777  0.23688282]\n",
        " [0.64391426 0.1748777  0.64391426]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tlaPl8PpcbE"
      },
      "source": [
        "### 3.2.2. Activation backward\n",
        "Next, you will need to implement the backward functions of `sigmoid()`, `relu()` and `softmax()`+`compute_CCE_cost`.\n",
        "\n",
        "**Exercise**: Implement backward function. (5%+5%) (basic: Sigmoid and ReLU, advanced: Softmax+CCE_loss)\n",
        "\n",
        "**Instruction**:\n",
        "*   sigmoid_backward: Implements the backward propagation for SIGMOID unit.\n",
        "*   relu_backward: Implements the backward propagation for RELU unit.\n",
        "*   softmax_CCE_backward: Implements the backward propagation for [SOFTMAX->CCE_LOSS] unit.\n",
        "\n",
        "If $g(.)$ is the activation function, sigmoid_backward, relu_backward and softmax_backward compute$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
        "\n",
        "1. The derivative of the sigmoid function is: $$σ^{'}(Z^{[l]}) = σ(Z^{[l]}) (1 - σ(Z^{[l]}))$$. <br>\n",
        "❗**Important**❗: You should use the numerically stable sigmoid function to prevent the overflow exponential problem. \n",
        "\n",
        "2. The derivative of the relu function is: $$g'(Z^{[l]}) = \\begin{cases}\n",
        "    1,& \\text{if } Z^{[l]}> 0\\\\\n",
        "    0,              & \\text{otherwise}\n",
        "\\end{cases}$$\n",
        "\n",
        "3. TLDR😉: The derivative of the categorical cross-entropy loss with respect to the last hidden layer is: $$\\frac{\\partial \\mathcal{L}}{\\partial Z} = s - y $$. <br> The derivative of the softmax function is: $$\\frac{\\partial S(z_i)}{\\partial z_j} = \\begin{cases}\n",
        "    S(z_i) \\times (1 - S(z_i)),& \\text{if } i = j\\\\\n",
        "    -S(z_i) \\times S(z_j),              & \\text{if } i \\neq j\n",
        "\\end{cases}$$, where $z$ is a vector with shape (number of classes K, 1) and $S(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$. Hence, the real derivative of softmax function would be a full Jacobian matrix. For the special case, K = 4, we have <img src=\"https://miro.medium.com/max/554/1*SWfgFQLDIPXDf1C6CHmr8A.png\" height=\"100\"/>. <br> It is quite complicated to calculate the softmax derivative on its own. However, if you use the softmax and the cross entropy loss, that complexity fades away. Since the softmax layer is usually used at the output, we can actually calculate the derivative of the categorical cross-entropy loss with respect to the n-th node in the last hidden layer. Instead of a long clunky formula, you end up with this terse, easy to compute thing: $$\\frac{\\partial \\mathcal{L}}{\\partial Z_i} = s_i - y_i $$, where $s$ is the output of the softmax function and the $y$ is the true label vector(one-hot vector). For more information, you can refer to this article [Derivative of the Softmax Function and the Categorical Cross-Entropy Loss](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1). <br> \n",
        "❗**Important**❗: The above mathematical derivation is based on naive implementation. In order to deal with the exponential overflow problem, we should use the normalized exponential function when counting $s$. For the sake of simplicity, we just use the same gradient equation as the naive implementation.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0p1wxIeBpcbF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sigmoid: dZ = [[-0.5        -0.26935835 -0.11969269 -0.5        -0.73139639]]\n",
            "ReLU: dZ = [[ 0.    0.   -1.14]\n",
            " [ 1.7   0.    3.72]]\n",
            "Softmax: dZ = [[-0.96488097  0.09003057  0.01766842]\n",
            " [ 0.70538451 -0.75527153  0.01766842]\n",
            " [ 0.25949646  0.66524096 -0.03533684]]\n"
          ]
        }
      ],
      "source": [
        "dA, cache = np.array([[-2, -1.37, -1.14, -2, -3.72]]), np.array([[0, 1, 2, 0, 1]])\n",
        "sigmoid = Activation(\"sigmoid\")\n",
        "sigmoid.cache = cache\n",
        "dZ = sigmoid.backward(dA=dA)\n",
        "print(\"Sigmoid: dZ = \"+ str(dZ))\n",
        "dA, cache = np.array([[-2, -2, -1.37, -1.14, -3.72]]), np.array([[2, 0, 1.5, 0, 0.5]])\n",
        "sigmoid.cache = cache\n",
        "output[\"sigmoid_backward\"] = sigmoid.backward(dA=dA)\n",
        "\n",
        "relu = Activation(\"relu\")\n",
        "dA, cache = np.array([[-2, -1.37, -1.14], [1.7, 2, 3.72]]), np.array([[-2, -1, 2], [1, 0, 1]])\n",
        "relu.cache = cache\n",
        "dZ = relu.backward(dA=dA)\n",
        "print(\"ReLU: dZ = \"+ str(dZ))\n",
        "dA, cache = np.array([[3.179, -1.376, -0.114], [2.227, -5.612, 4.172]]), np.array([[0.53, 1.21, -2.22], [-1.58, 0.99, -0.11]])\n",
        "relu.cache = cache\n",
        "output[\"relu_backward\"] = relu.backward(dA=dA)\n",
        "\n",
        "Y, cache = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), np.array([[-2, -1, -2], [1, 0, -2], [0, 1, 2]])\n",
        "softmax = Activation(\"softmax\")\n",
        "softmax.cache = cache\n",
        "dZ = softmax.backward(Y=Y)\n",
        "print(\"Softmax: dZ = \" + str(dZ))\n",
        "Y, cache = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), np.array([[-2.11, -1.22, -2.33], [1.44, 0.55, -2.66], [0.77, 1.88, 2.99]])\n",
        "softmax.cache = cache\n",
        "output[\"softmax_CCE_backward\"] = softmax.backward(Y=Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwYDe3WfpcbF"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>(With sigmoid) dZ: </td>\n",
        "    <td>[[-0.5        -0.26935835 -0.11969269 -0.5        -0.73139639]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(With ReLU) dZ: </td>\n",
        "    <td>[[ 0.    0.   -1.14]\n",
        " [ 1.7   0.    3.72]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(With softmax) dZ: </td>\n",
        "    <td>[[-0.96488097  0.09003057  0.01766842]\n",
        " [ 0.70538451 -0.75527153  0.01766842]\n",
        " [ 0.25949646  0.66524096 -0.03533684]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYqpQu6Eye7h"
      },
      "source": [
        "## 3.3. Model\n",
        "Alright, now you have all the tools that are needed to build a model. Let's get started! 😀\n",
        "\n",
        "### 3.3.1. Model initialize parameters\n",
        "First, you will need to initialize your model by creating several linear and activation function layers. \n",
        "\n",
        "**Exercise**: Implement model initialize parameters. (5%)\n",
        "\n",
        "**Instruction**:\n",
        "*   Use the functions you had previously written.\n",
        "*   Store all the linear layers in a list called linear.\n",
        "*   Store all the activation function layers in a list called activation.\n",
        "\n",
        "❗**Important**❗: We set the random seed for grading purposes to keep all the random function calls consistent. However, we still want all the linear layers to have different initialized weights, so when implementing this function, please make sure that you pass the number of iterations as the seed number to the Dense layer initialization call.\n",
        "\n",
        "**Note**: In deep learning, a linear-activation layer is counted as a single layer in the neural network, not two layers since the activation layer does not have any parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0JGMzfIDCSVz"
      },
      "outputs": [],
      "source": [
        "class Model():\n",
        "    def __init__(self, units, activation_functions):\n",
        "        self.units = units\n",
        "        self.activation_functions = activation_functions\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "        self.units -- number of nodes/units for each layer, starting from the input dimension and ending with the output dimension (i.e., [4, 4, 1])\n",
        "        self.activation_functions -- activation functions used in each layer (i.e, [\"relu\", \"sigmoid\"])\n",
        "        self.linear -- a list to store the dense layers when initializing the model\n",
        "        self.activation -- a list to store the activation function layers when initializing the model\n",
        "        \"\"\"\n",
        "        self.linear = []\n",
        "        self.activation = []\n",
        "\n",
        "        # GRADED FUNCTION: model_initialize_parameters\n",
        "        ### START CODE HERE ### (≈ 5 lines of code)\n",
        "        n_layers = len(self.units)-1\n",
        "        for i in range(n_layers): self.linear.append(Dense(self.units[i], self.units[i+1], i))\n",
        "        for functions in self.activation_functions: self.activation.append(Activation(functions))\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "        X -- input data: (number of features, number of examples)\n",
        "        \n",
        "        Returns:\n",
        "        A -- output of L-layer neural network, probability vector corresponding to your label predictions, shape (number of classes, number of examples)\n",
        "        \"\"\"\n",
        "        A = X\n",
        "\n",
        "        # GRADED FUNCTION: model_forward\n",
        "        ### START CODE HERE ### (≈ 4 lines of code)\n",
        "        n_layers = len(self.linear)\n",
        "        for i in range(n_layers):\n",
        "            A = self.linear[i].forward(A)\n",
        "            A = self.activation[i].forward(A)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, AL=None, Y=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "        For multi-class classification,\n",
        "        AL -- output of L-layer neural network, probability vector corresponding to your label predictions, shape (number of classes, number of examples)\n",
        "        Y -- true \"label\" vector (one hot vector, for example: [[1], [0], [0]] represents rock, [[0], [1], [0]] represents paper, [[0], [0], [1]] represents scissors \n",
        "                              in a Rock-Paper-Scissors image classification), shape (number of classes, number of examples)\n",
        "\n",
        "        Returns:\n",
        "        dA_prev -- post-activation gradient\n",
        "        \"\"\"\n",
        "\n",
        "        L = len(self.linear)\n",
        "\n",
        "        # GRADED FUNCTION: model_backward\n",
        "        ### START CODE HERE ### (≈ 10 lines of code)\n",
        "        if self.activation_functions[-1] == \"sigmoid\":\n",
        "            # Initializing the backpropagation\n",
        "            e = 1e-5\n",
        "            dAL = - ((np.divide(Y, AL+e)) - (np.divide(1-Y, 1-AL+e)))\n",
        "            dZ = self.activation[L-1].backward(dAL, Y)\n",
        "            dA_prev = self.linear[L-1].backward(dZ)\n",
        "            # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
        "        else:\n",
        "            # Initializing the backpropagation\n",
        "            dZ = self.activation[L-1].backward(AL, Y)\n",
        "\n",
        "            # Lth layer (LINEAR) gradients. Inputs: \"dZ\". Outputs: \"dA_prev\"\n",
        "            dA_prev = self.linear[L-1].backward(dZ)\n",
        "\n",
        "        # Loop from l=L-2 to l=0\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        # Inputs: \"dA_prev\". Outputs: \"dA_prev\"\n",
        "        for layer in range(L-2, -1, -1):\n",
        "            dZ = self.activation[layer].backward(dA_prev, Y)\n",
        "            dA_prev = self.linear[layer].backward(dZ)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        return dA_prev\n",
        "\n",
        "    def update(self, learning_rate):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "        learning_rate -- step size\n",
        "        \"\"\"\n",
        "\n",
        "        L = len(self.linear)\n",
        "\n",
        "\n",
        "        # GRADED FUNCTION: model_update_parameters\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        for i in range(L): self.linear[i].update(learning_rate)\n",
        "        ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EGY7_1bjcm-c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W1:  [[ 0.09762701  0.43037873  0.20552675]\n",
            " [ 0.08976637 -0.1526904   0.29178823]\n",
            " [-0.12482558  0.783546    0.92732552]] \n",
            "b1:  [[0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "W2:  [[-0.20325375  0.53968259 -1.22446471]] \n",
            "b2:  [[0.]]\n"
          ]
        }
      ],
      "source": [
        "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"])\n",
        "print(\"W1: \", model.linear[0].parameters[\"W\"], \"\\nb1: \", model.linear[0].parameters[\"b\"])\n",
        "print(\"W2: \", model.linear[1].parameters[\"W\"], \"\\nb2: \", model.linear[1].parameters[\"b\"])\n",
        "\n",
        "model = Model([16, 8, 1], [\"relu\", \"sigmoid\"])\n",
        "output[\"model_initialize_parameters\"] = (model.linear[0].parameters, model.linear[1].parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEmggOxtdMnl"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>W1: </td>\n",
        "    <td>[[ 0.09762701  0.43037873  0.20552675]\n",
        " [ 0.08976637 -0.1526904   0.29178823]\n",
        " [-0.12482558  0.783546    0.92732552]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b1: </td>\n",
        "    <td>[[0.]\n",
        " [0.]\n",
        " [0.]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>W2: </td>\n",
        "    <td>[[-0.20325375  0.53968259 -1.22446471]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b2: </td>\n",
        "    <td>[[0.]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJVlZeyNAu-y"
      },
      "source": [
        "### 3.3.2. Model forward\n",
        "\n",
        "After that, you will implement the model forward function by calling the forward function of each layer in the linear and activation function layer you have created in the previous step.\n",
        "\n",
        "For a $N$-layer neural network, you will call the forward function of the linear layers and then followed by the activation function layers for $N-1$ times. The last activation function layer will be sigmoid for binary classification and softmax for multi-class classification.\n",
        "\n",
        "**Exercise**: Implement model forward. (5%)\n",
        "\n",
        "**Instruction**:\n",
        "*   Use the functions you had previously written.\n",
        "*   Use a for loop to replicate [LINEAR->ACTIVATION] (N-1) times.\n",
        "\n",
        "**Note**: There are K nodes in the last layer for K-class classification, but only one node for binary classification. Intuitively, this could be pretty confusing sometimes since there should be two nodes in the last layer for binary classification. However, both the one-node(sigmoid, binary cross-entropy) and two-node(softmax, categorical cross-entropy) techniques for binary classification work fine, and picking one technique over the other is a matter of subjective preference. For this assignment, you will implement the former one, which is what we usually do for binary classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6yVQQqe2EyHA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With sigmoid: A = [[0.64565631 0.20915937 0.77902611]]\n",
            "With ReLU: A = [[0.6  0.   1.26]]\n",
            "With softmax: A = \n",
            "[[0.47535001 0.05272708 0.68692136]\n",
            " [0.14317267 0.75380161 0.05526942]\n",
            " [0.38147732 0.19347131 0.25780921]]\n"
          ]
        }
      ],
      "source": [
        "A_prev, W, b = np.array([[0.1, -1.2, 1.9], [1.1, 0.2, 2.3], [2.9, -2.5, 3.7]]), np.array([[0.1, 0.2, 0.3]]), np.array([[-0.5]])\n",
        "model = Model([3, 1], [\"sigmoid\"])\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "A = model.forward(A_prev)\n",
        "print(\"With sigmoid: A = \" + str(A))\n",
        "A_prev, W, b = np.array([[1.1, -2.2], [-3.9, 0.6]]), np.array([[9.1, -8.2]]), np.array([[0.5]])\n",
        "model = Model([2, 1], [\"sigmoid\"])\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "A = model.forward(A_prev)\n",
        "output[\"model_forward_sigmoid\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
        "\n",
        "A_prev, W, b = np.array([[0.1, -1.2, 1.9], [1.1, 0.2, 2.3], [2.9, -2.5, 3.7]]), np.array([[0.1, 0.2, 0.3]]), np.array([[-0.5]])\n",
        "model = Model([3, 1], [\"relu\"])\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "A = model.forward(A_prev)\n",
        "print(\"With ReLU: A = \" + str(A))\n",
        "A_prev, W, b = np.array([[1.1, -2.2], [-3.9, 0.6]]), np.array([[9.1, -8.2]]), np.array([[0.5]])\n",
        "model = Model([2, 1], [\"relu\"])\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "A = model.forward(A_prev)\n",
        "output[\"model_forward_relu\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
        "\n",
        "A_prev, W, b = np.array([[0.1, -1.2, 1.9], [1.1, 0.2, 2.3], [2.9, -2.5, 3.7]]), np.array([[0.1, 0.2, 0.3], [-0.1, -0.2, -0.3], [-0.1, 0, 0.1]]), np.array([[-0.5], [0.5], [0.1]])\n",
        "model = Model([3, 3], [\"softmax\"])\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "A = model.forward(A_prev)\n",
        "print(\"With softmax: A = \\n\" + str(A))\n",
        "A_prev, W, b = np.array([[-0.1, 1.2, 1.9], [-1.1, 0.2, -2.3], [2.9, -2.5, -3.7]]), np.array([[0.2, 0.2, 0.2], [-0.1, -0.1, -0.1], [-0.1, 0, 0.1]]), np.array([[-0.1], [0.1], [0.5]])\n",
        "model = Model([3, 3], [\"softmax\"])\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "A = model.forward(A_prev)\n",
        "output[\"model_forward_softmax\"] = (A, (model.linear[0].cache, model.activation[0].cache))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMkf2ss6F52W"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>(With sigmoid) A: </td>\n",
        "    <td>[[0.64565631 0.20915937 0.77902611]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(With ReLU) A: </td>\n",
        "    <td>[[0.6  0.   1.26]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(With softmax) A: </td>\n",
        "    <td>[[0.47535001 0.05272708 0.68692136]\n",
        " [0.14317267 0.75380161 0.05526942]\n",
        " [0.38147732 0.19347131 0.25780921]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "s26LVkCbIbJ3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AL = [[0.56058713 0.55220559 0.46331713]]\n",
            "Length of layers list = 2\n",
            "AL = [[0.11637212 0.11413265 0.09750771]\n",
            " [0.08186754 0.08432761 0.07419482]\n",
            " [0.0924809  0.09365443 0.08444682]\n",
            " [0.09675205 0.09736489 0.10943351]\n",
            " [0.12819411 0.12404237 0.09669465]\n",
            " [0.09664001 0.09726785 0.11116299]\n",
            " [0.08448599 0.08664355 0.08734059]\n",
            " [0.09067641 0.09207969 0.12452515]\n",
            " [0.1294968  0.12512634 0.13002144]\n",
            " [0.08303407 0.08536063 0.08467232]]\n",
            "Length of layers list = 2\n"
          ]
        }
      ],
      "source": [
        "# binary classification\n",
        "X = np.array([[0, 1, 2], [-2, -1, 0], [0.5, 0.5, 0.5]])\n",
        "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"])\n",
        "AL = model.forward(X)\n",
        "print(\"AL = \" + str(AL))\n",
        "print(\"Length of layers list = \" + str(len(model.linear)))\n",
        "\n",
        "# multi-class classification\n",
        "X = np.array([[0, 1, 2], [-2, -1, 0], [0.5, 0.5, 0.5]])\n",
        "model = Model([3, 3, 10], [\"relu\", \"softmax\"])\n",
        "AL = model.forward(X)\n",
        "print(\"AL = \" + str(AL))\n",
        "print(\"Length of layers list = \" + str(len(model.linear)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoCdrONOHhvw"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>(Binary classification) AL: </td>\n",
        "    <td>[[0.56058713 0.55220559 0.46331713]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(Binary classification) Length of layers list: </td>\n",
        "    <td>2</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(Multi-class classification) AL: </td>\n",
        "    <td>[[0.11637212 0.11413265 0.09750771]\n",
        " [0.08186754 0.08432761 0.07419482]\n",
        " [0.0924809  0.09365443 0.08444682]\n",
        " [0.09675205 0.09736489 0.10943351]\n",
        " [0.12819411 0.12404237 0.09669465]\n",
        " [0.09664001 0.09726785 0.11116299]\n",
        " [0.08448599 0.08664355 0.08734059]\n",
        " [0.09067641 0.09207969 0.12452515]\n",
        " [0.1294968  0.12512634 0.13002144]\n",
        " [0.08303407 0.08536063 0.08467232]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(Multi-class classification) Length of layers list: </td>\n",
        "    <td>2</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPBl7iq7N2wY"
      },
      "source": [
        "###3.3.3. Model backward\n",
        "Now you will implement the backward function for the whole network. Recall that you have implemented the backward function for the dense and activation function layer. In this section, you will call these functions to help you implement the model backward function. You will iterate through all the hidden layers backward, starting from layer $L$. On each step, you will call the backward function of layer $l$ to backpropagate through layer $l$.\n",
        "\n",
        "**Exercise**: Implement model backward. (5%)\n",
        "\n",
        "**Instruction**:\n",
        "*   Use the functions you had previously written.\n",
        "*   Initialize backpropagation.\n",
        "*   Use a for loop to backprop from layer $L-1$ to layer $1$.\n",
        "\n",
        "Initializing backpropagation:\n",
        "\n",
        "(1) Binary classification: To backpropagate through this network, we know that the output is, $A^{[L]} = \\sigma(Z^{[L]})$. Your code thus needs to compute dAL $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$. To do so, use this formula (derived using calculus which you don't need in-depth knowledge of):\n",
        "```\n",
        "dAL = - (np.divide(Y, AL + ϵ) - np.divide(1 - Y, 1 - AL + ϵ)) # derivative of cost with respect to AL, where ϵ = 1e-5 is added to prevent zero division.\n",
        "```\n",
        "\n",
        "You can then use this post-activation gradient dAL to keep going backward. You can now feed in dAL into the LINEAR->SIGMOID backward function you implemented (which will use the cached values stored inside each layer in the forward pass). After that, you will have to use a for loop to iterate through all the other layers using the LINEAR->RELU backward function. \n",
        "\n",
        "(2) Multi-class classification: Since you have implemented the backward function of the softmax activation function layer along with the categorical cross-entropy loss, you can directly call the softmax_CCE_backward function implemented inside the activation function layer and followed by the linear backward function to obtain the post-activation gradient to keep going backward. After that, you will have to use a for loop to iterate through all the other layers using the LINEAR->RELU backward function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HOGsyLXPNGh5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sigmoid:\n",
            "dA_prev = [[ 0.55554938  0.49152369 -0.41996594 -0.55554938 -0.39321993]\n",
            " [ 0.27777469  0.24576184 -0.20998297 -0.27777469 -0.19660997]]\n",
            "dW = [[-0.29446117  0.29446117]]\n",
            "db = [[-0.03216622]]\n",
            "\n",
            "relu:\n",
            "dA_prev = [[-0.01269296  0.01470136  0.         -0.07496777 -0.07151883]\n",
            " [-0.05595562  0.06480946  0.         -0.0327431  -0.03123674]]\n",
            "dW = [[ 0.0178719  -0.0178719 ]\n",
            " [-0.17321413  0.17321413]]\n",
            "db = [[ 0.00335943]\n",
            " [-0.11638953]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "AL, Y, linear_activation_cache  = np.array([[0.1, 0.2, 0.5, 0.9, 1.0]]), np.array([[0, 0, 1, 1, 1]]), ((np.array([[-2, -1, 0, 1, 2], [2, 1, 0, -1, -2]]), np.array([[2.0, 1.0]]), np.array([[0.5]])), np.array([[0, 1, 2, 0, 1]]))\n",
        "model = Model([2, 1], [\"sigmoid\"])\n",
        "model.linear[0].cache = linear_activation_cache[0]\n",
        "model.activation[0].cache = linear_activation_cache[1]\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "print (\"sigmoid:\")\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(model.linear[0].dW))\n",
        "print (\"db = \" + str(model.linear[0].db) + \"\\n\")\n",
        "\n",
        "AL, Y, linear_activation_cache  = np.array([[0.15, 0.23, 0.79, 0.97, 0.99]]), np.array([[0, 0, 1, 1, 1]]), ((np.array([[-2, -1, 0, 1, 2], [2, 1, 0, -1, -2]]), np.array([[2.0, 1.0]]), np.array([[0.5]])), np.array([[0, 1, 2, 0, 1]]))\n",
        "model = Model([2, 1], [\"sigmoid\"])\n",
        "model.linear[0].cache = linear_activation_cache[0]\n",
        "model.activation[0].cache = linear_activation_cache[1]\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "output[\"model_backward_sigmoid\"] = (dA_prev, model.linear[0].dW, model.linear[0].db) \n",
        "\n",
        "X, Y = np.array([[-2, -1, 0, 1, 2], [2, 1, 0, -1, -2]]), np.array([[0, 1, 1, 1, 1]])\n",
        "model = Model([2, 2, 1], [\"relu\", \"sigmoid\"])\n",
        "AL = model.forward(X)\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "print (\"relu:\")\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(model.linear[0].dW))\n",
        "print (\"db = \" + str(model.linear[0].db) + \"\\n\")\n",
        "\n",
        "X, Y = np.array([[-2.5, -1.3, 0.1, 1.9, 2.7], [1.2, 2.1, 3.0, -4.1, -5.2]]), np.array([[1, 1, 0, 0, 0]])\n",
        "model = Model([2, 2, 1], [\"relu\", \"sigmoid\"])\n",
        "AL = model.forward(X)\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "output[\"model_backward_relu\"] = (dA_prev, model.linear[0].dW, model.linear[0].db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6xzEk3-NGh6"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Sigmoid </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dA_prev: </td>\n",
        "    <td>[[ 0.55554938  0.49152369 -0.41996594 -0.55554938 -0.39321993]\n",
        " [ 0.27777469  0.24576184 -0.20998297 -0.27777469 -0.19660997]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW: </td>\n",
        "    <td>[[-0.29446117  0.29446117]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db: </td>\n",
        "    <td>[[-0.03216622]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>ReLU </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dA_prev: </td>\n",
        "    <td>[[-0.01269296  0.01470136  0.         -0.07496777 -0.07151883]\n",
        " [-0.05595562  0.06480946  0.         -0.0327431  -0.03123674]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW: </td>\n",
        "    <td>[[ 0.0178719  -0.0178719 ]\n",
        " [-0.17321413  0.17321413]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db: </td>\n",
        "    <td>[[ 0.00335943]\n",
        " [-0.11638953]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BC1QnMSKN2wZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary classification\n",
            "dW1 = [[-0.06277946  0.         -0.01569486]\n",
            " [ 0.26602938  0.05875647  0.05181823]\n",
            " [-0.37820327  0.         -0.09455082]]\n",
            "db1 = [[-0.03138973]\n",
            " [ 0.10363646]\n",
            " [-0.18910163]]\n",
            "dA_prev = [[-0.02128713  0.02675119  0.08406585]\n",
            " [ 0.03620889 -0.04550313 -0.52321654]\n",
            " [-0.06919444  0.08695554 -0.47247201]]\n",
            "\n",
            "Multi-class classification\n",
            "dW1 = [[ 0.16593371  0.          0.04148343]\n",
            " [ 0.33171007  0.15006987  0.04541005]\n",
            " [-0.32297709  0.         -0.08074427]]\n",
            "db1 = [[ 0.08296685]\n",
            " [ 0.0908201 ]\n",
            " [-0.16148854]]\n",
            "dA_prev = [[-0.04735391  0.05429414  0.10229066]\n",
            " [ 0.08054785 -0.09235301 -0.30227651]\n",
            " [-0.15392528  0.1764847  -0.34116033]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# binary classification\n",
        "X, Y = np.array([[0, 1, 2], [-2, -1, 0], [0.5, 0.5, 0.5]]), np.array([[1, 0, 0]])\n",
        "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"])\n",
        "AL = model.forward(X)\n",
        "\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "print(\"Binary classification\")\n",
        "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
        "print(\"db1 = \"+ str(model.linear[0].db))\n",
        "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")\n",
        "\n",
        "# multi-class classification\n",
        "X, Y= np.array([[0, 1, 2], [-2, -1, 0], [0.5, 0.5, 0.5]]), np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
        "model = Model([3, 3, 3], [\"relu\", \"softmax\"])\n",
        "AL = model.forward(X)\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "print(\"Multi-class classification\")\n",
        "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
        "print(\"db1 = \"+ str(model.linear[0].db))\n",
        "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cYzCzY8N2wZ"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Binary classification </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW1: </td>\n",
        "    <td>[[-0.06277946  0.         -0.01569486]\n",
        " [ 0.26602938  0.05875647  0.05181823]\n",
        " [-0.37820327  0.         -0.09455082]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db1: </td>\n",
        "    <td>[[-0.03138973]\n",
        " [ 0.10363646]\n",
        " [-0.18910163]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dA1: </td>\n",
        "    <td>[[-0.02128713  0.02675119  0.08406585]\n",
        " [ 0.03620889 -0.04550313 -0.52321654]\n",
        " [-0.06919444  0.08695554 -0.47247201]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Multi-class classification </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW1: </td>\n",
        "    <td>[[ 0.16593371  0.          0.04148343]\n",
        " [ 0.33171007  0.15006987  0.04541005]\n",
        " [-0.32297709  0.         -0.08074427]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db1: </td>\n",
        "    <td>[[ 0.08296685]\n",
        " [ 0.0908201 ]\n",
        " [-0.16148854]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dA1: </td>\n",
        "    <td>[[-0.04735391  0.05429414  0.10229066]\n",
        " [ 0.08054785 -0.09235301 -0.30227651]\n",
        " [-0.15392528  0.1764847  -0.34116033]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wiJu3YlUCc7"
      },
      "source": [
        "# 3.3.4. Model update parameters\n",
        "In this section you will update the parameters of the model, using gradient descent:\n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$\n",
        "where $\\alpha$ is the learning rate.\n",
        "\n",
        "**Exercise**: Implement update() to update your parameters using gradient descent. (5%)\n",
        "\n",
        "**Instructions**: \n",
        "*   Use the functions you had previously written.\n",
        "*   Update parameters using gradient descent on every $W^{[l]}$ and $b^{[l]}$ for $l = 1, 2, ..., L$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qoGA4O8BUCvq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W1 = [[ 0.39721186  0.64025004 -0.09671178  0.27099015]\n",
            " [ 0.07752363  0.00469968  0.09679955  0.33705631]\n",
            " [ 0.392862    0.52183369  0.33138026  0.67538482]]\n",
            "b1 = [[ 0.16234149]\n",
            " [ 0.78232848]\n",
            " [-0.02592894]]\n",
            "W2 = [[0.6012798  0.38575324 0.49003974]]\n",
            "b2 = [[0.05692437]]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1)\n",
        "parameters, grads = {\"W1\": np.random.rand(3, 4), \"b1\": np.random.rand(3,1), \"W2\": np.random.rand(1,3), \"b2\": np.random.rand(1,1)}, {\"dW1\": np.random.rand(3, 4), \"db1\": np.random.rand(3,1), \"dW2\": np.random.rand(1,3), \"db2\": np.random.rand(1,1)}\n",
        "model = Model([4, 3, 1], [\"relu\", \"sigmoid\"])\n",
        "model.linear[0].parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
        "model.linear[1].parameters = {\"W\": parameters[\"W2\"], \"b\": parameters[\"b2\"]}\n",
        "model.linear[0].dW, model.linear[0].db, model.linear[1].dW, model.linear[1].db = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n",
        "model.update(0.1)\n",
        "\n",
        "print (\"W1 = \"+ str(model.linear[0].parameters[\"W\"]))\n",
        "print (\"b1 = \"+ str(model.linear[0].parameters[\"b\"]))\n",
        "print (\"W2 = \"+ str(model.linear[1].parameters[\"W\"]))\n",
        "print (\"b2 = \"+ str(model.linear[1].parameters[\"b\"]))\n",
        "\n",
        "np.random.seed(1)\n",
        "parameters, grads = {\"W1\": np.random.randn(3, 4), \"b1\": np.random.randn(3,1), \"W2\": np.random.randn(1,3), \"b2\": np.random.randn(1,1)}, {\"dW1\": np.random.randn(3, 4), \"db1\": np.random.randn(3,1), \"dW2\": np.random.randn(1,3), \"db2\": np.random.randn(1,1)}\n",
        "model = Model([4, 3, 1], [\"relu\", \"sigmoid\"])\n",
        "model.linear[0].parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
        "model.linear[1].parameters = {\"W\": parameters[\"W2\"], \"b\": parameters[\"b2\"]}\n",
        "model.linear[0].dW, model.linear[0].db, model.linear[1].dW, model.linear[1].db = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n",
        "model.update(0.075)\n",
        "output[\"model_update_parameters\"] = {\"W1\": model.linear[0].parameters[\"W\"], \"b1\": model.linear[0].parameters[\"b\"], \"W2\": model.linear[1].parameters[\"W\"], \"b2\": model.linear[1].parameters[\"b\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t-HfnHZWYIa"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>W1: </td>\n",
        "    <td>[[ 0.39721186  0.64025004 -0.09671178  0.27099015]\n",
        " [ 0.07752363  0.00469968  0.09679955  0.33705631]\n",
        " [ 0.392862    0.52183369  0.33138026  0.67538482]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b1: </td>\n",
        "    <td>[[ 0.16234149]\n",
        " [ 0.78232848]\n",
        " [-0.02592894]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>W2: </td>\n",
        "    <td>[[0.6012798  0.38575324 0.49003974]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b2: </td>\n",
        "    <td>[[0.05692437]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmSBVaQOSRrk"
      },
      "source": [
        "# 4. Cost function\n",
        "In this section, you will implement the cost function. We use binary cross-entropy loss for binary classification and categorical cross-entropy loss for multi-class classification. You need to compute the cost, because you want to check if your model is actually learning. Cross-entropy loss is minimized, where smaller values represent a better model than larger values. A model that predicts perfect probabilities has a cross entropy or log loss of 0.0.\n",
        "\n",
        "## 4.1. Binary cross-entropy loss\n",
        "**Exercise**: Compute the binary cross-entropy cost $J$, using the following formula: (5%) $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}+ϵ\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}+ϵ\\right)), where\\ ϵ=1e-5$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MjBT0eYQaY81"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: compute_BCE_cost\n",
        "\n",
        "def compute_BCE_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the binary cross-entropy cost function using the above formula.\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- binary cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    cost = -1/m * np.sum((Y*np.log(AL+1e-5))+(1-Y)*np.log(1-AL+1e-5))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "r07sqnIXaaMv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cost = 0.5783820772863568\n"
          ]
        }
      ],
      "source": [
        "AL, Y = np.array([[0.9, 0.6, 0.4, 0.1, 0.2, 0.8]]), np.array([[1, 1, 1, 0, 0, 0]])\n",
        "\n",
        "print(\"cost = \" + str(compute_BCE_cost(AL, Y)))\n",
        "output[\"compute_BCE_cost\"] = compute_BCE_cost(np.array([[0.791, 0.983, 0.654, 0.102, 0.212, 0.091, 0.476, 0.899]]), np.array([[1, 1, 1, 1, 0, 0, 0, 0]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iRtgOx_IGPo"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>cost: </td>\n",
        "    <td>0.5783820772863568</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aealRyKbcQzG"
      },
      "source": [
        "## 4.2. Categorical cross-entropy loss\n",
        "**Exercise**: Compute the categorical cross-entropy cost $J$, using the following formula: (5%) $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}+ϵ\\right)), where\\ ϵ = 1e-5$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Owx-kTdcfxV5"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: compute_CCE_cost\n",
        "\n",
        "def compute_CCE_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the categorical cross-entropy cost function using the above formula.\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (number of classes, number of examples)\n",
        "    Y -- true \"label\" vector (one hot vector, for example: [[1], [0], [0]] represents rock, [[0], [1], [0]] represents paper, [[0], [0], [1]] represents scissors \n",
        "                              in a Rock-Paper-Scissors image classification), shape (number of classes, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- categorical cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    cost = -1/m * np.sum(Y*np.log(AL+1e-5))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0YbHVAc7hSh3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cost = 0.4722526144672341\n"
          ]
        }
      ],
      "source": [
        "AL, Y = np.array([[0.8, 0.6, 0.4, 0.1, 0.2, 0.4], [0.1, 0.3, 0.5, 0.7, 0.1, 0.1], [0.1, 0.1, 0.1, 0.2, 0.7, 0.5]]), np.array([[1, 1, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 1, 1]])\n",
        "print(\"cost = \" + str(compute_CCE_cost(AL, Y)))\n",
        "output[\"compute_CCE_cost\"] = compute_CCE_cost(np.array([[0.711, 0.001, 0.11], [0.099, 0.217, 0.09], [0.035, 0.599, 0.12], [0.068, 0.123, 0.1], [0.087, 0.06, 0.58]]), np.array([[1, 0, 0], [0, 0, 0], [0, 1, 0], [0, 0, 0], [0, 0, 1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9VVIBB5Ic-D"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>cost: </td>\n",
        "    <td>0.4722526144672341</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpQah0JDdMyl"
      },
      "source": [
        "# Basic implementation (binary classification)\n",
        "\n",
        "Congratulations on implementing all the functions by yourself. You have done an incredible job! 👏\n",
        "\n",
        "Now you have all the tools you need to get started with classification. In this section, you will build a binary classifier using the functions you had previously written. You will create a model that can determine whether breast cancer is malignant or benign based on 30 features. Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe the characteristics of the cell nuclei present in the image.\n",
        "\n",
        "Ten real-valued features are computed for each cell nucleus:\n",
        "\n",
        "1.   radius (mean of distances from center to points on the perimeter)\n",
        "2.   texture (standard deviation of gray-scale values)\n",
        "3.   perimeter\n",
        "4.   area\n",
        "5.   smoothness (local variation in radius lengths)\n",
        "6.   compactness (perimeter^2 / area - 1.0)\n",
        "7.   concavity (severity of concave portions of the contour)\n",
        "8.   concave points (number of concave portions of the contour)\n",
        "9.   symmetry\n",
        "10.   fractal dimension (\"coastline approximation\" - 1)\n",
        "\n",
        "The mean, standard error and \"worst\" or largest (mean of the three\n",
        "largest values) of these features were computed for each image,\n",
        "resulting in 30 features. For instance, field 3 is Mean Radius, field\n",
        "13 is Radius SE, field 23 is Worst Radius.\n",
        "\n",
        "**Exercise**: Implement a binary classifier and tune hyperparameter. (10%)\n",
        "\n",
        "**Instruction**:\n",
        "*   Use the functions you had previously written.\n",
        "*   Preprocess the data by using min-max scaling to normalize X. Normalize the values of each feature between 0 and 1.\n",
        "*   Use batch gradient descent to train the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5OzSS4zFHezi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of X: (30, 500)\n",
            "shape of y: (1, 500)\n",
            "shape of X_train: (30, 400) shape of y_train: (1, 400)\n",
            "shape of X_val: (30, 100) shape of y_val: (1, 100)\n"
          ]
        }
      ],
      "source": [
        "# load breast cancer wisconsin dataset\n",
        "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
        "X = X[:500].T\n",
        "y = np.expand_dims(y[:500], axis=1).T\n",
        "\n",
        "print(\"shape of X: \" + str(X.shape))\n",
        "print(\"shape of y: \" + str(y.shape))\n",
        "\n",
        "# GRADED CODE: binary classification\n",
        "### START CODE HERE ###\n",
        "# min max scaling\n",
        "n_rows = X.shape[0]\n",
        "for row in range(n_rows) :\n",
        "    X[row] = (X[row] - X[row].min()) / (X[row].max() - X[row].min())\n",
        "### END CODE HERE ###\n",
        "\n",
        "# split training set and validation set\n",
        "X_train, y_train = X[:, :400], y[:, :400]\n",
        "X_val, y_val = X[:, 400:], y[:, 400:]\n",
        "\n",
        "print(\"shape of X_train: \" + str(X_train.shape) + \" shape of y_train: \" + str(y_train.shape))\n",
        "print(\"shape of X_val: \" + str(X_val.shape) + \" shape of y_val: \" + str(y_val.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fI7JY5ESjhZ2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.690172\n",
            "Cost after iteration 100: 0.167676\n",
            "Cost after iteration 200: 0.083511\n",
            "Cost after iteration 300: 0.067453\n",
            "Cost after iteration 400: 0.060116\n",
            "Cost after iteration 500: 0.055312\n",
            "Cost after iteration 600: 0.051655\n",
            "Cost after iteration 700: 0.048723\n",
            "Cost after iteration 800: 0.046257\n",
            "Cost after iteration 900: 0.044154\n",
            "Cost after iteration 1000: 0.042594\n",
            "Cost after iteration 1100: 0.041029\n",
            "Cost after iteration 1200: 0.078070\n",
            "Cost after iteration 1300: 0.039471\n",
            "Cost after iteration 1400: 0.037138\n",
            "Cost after iteration 1500: 0.038217\n",
            "Cost after iteration 1600: 0.052816\n",
            "Cost after iteration 1700: 0.035370\n",
            "Cost after iteration 1800: 0.032474\n",
            "Cost after iteration 1900: 0.031370\n",
            "Cost after iteration 2000: 0.030306\n",
            "Cost after iteration 2100: 0.029801\n",
            "Cost after iteration 2200: 0.046789\n",
            "Cost after iteration 2300: 0.039040\n",
            "Cost after iteration 2400: 0.026187\n",
            "Cost after iteration 2500: 0.024867\n",
            "Cost after iteration 2600: 0.023821\n",
            "Cost after iteration 2700: 0.022766\n",
            "Cost after iteration 2800: 0.021432\n",
            "Cost after iteration 2900: 0.020267\n",
            "Cost after iteration 3000: 0.019084\n",
            "Cost after iteration 3100: 0.018438\n",
            "Cost after iteration 3200: 0.016641\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# GRADED CODE: binary classification\n",
        "### START CODE HERE ###\n",
        "layers_dims = [30, 108, 54, 27, 1]\n",
        "activation_fn = [\"relu\", \"relu\", \"relu\", \"sigmoid\"]\n",
        "learning_rate = 0.0993652\n",
        "num_iterations = 3269     #repeats\n",
        "print_cost = True\n",
        "classes = 2\n",
        "costs = []                        # keep track of cost\n",
        "model = Model(layers_dims, activation_fn)\n",
        "\n",
        "# Loop (batch gradient descent)\n",
        "for i in range(0, num_iterations):\n",
        "    # forward\n",
        "    AL = model.forward(X_train)\n",
        "\n",
        "    # compute cost\n",
        "    if classes == 2:\n",
        "        cost = compute_BCE_cost(AL, y_train)\n",
        "    else:\n",
        "        cost = compute_CCE_cost(AL, y_train)\n",
        "\n",
        "    # backward\n",
        "    dA_prev = model.backward(AL, y_train)\n",
        "    \n",
        "    # update\n",
        "    model.update(learning_rate)\n",
        "\n",
        "    if print_cost and i % 100 == 0:\n",
        "        print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        costs.append(cost)\n",
        "            \n",
        "# plot the cost\n",
        "plt.plot(np.squeeze(costs))\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "plt.show()\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "woCqucFUYXe6"
      },
      "outputs": [],
      "source": [
        "# Helper function\n",
        "def predict(X, y, model, classes):\n",
        "    \"\"\"\n",
        "    This function is used to predict the results of a  L-layer neural network.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data set of examples you would like to label\n",
        "    model -- trained model\n",
        "    classes - number of classes, 2 for binary classification, >2 for multi-class classification\n",
        "    \n",
        "    Returns:\n",
        "    p -- predictions for the given dataset X\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    n = len(model.linear) # number of layers in the neural network\n",
        "\n",
        "    if classes == 2:\n",
        "      p = np.zeros((1,m))\n",
        "    else:\n",
        "      p = np.zeros((classes, m))\n",
        "    \n",
        "    # Forward propagation\n",
        "    probas = model.forward(X)\n",
        "    \n",
        "    if classes == 2:\n",
        "      # convert probas to 0/1 predictions\n",
        "      for i in range(0, probas.shape[1]):\n",
        "          if probas[0,i] > 0.5:\n",
        "              p[0,i] = 1\n",
        "          else:\n",
        "              p[0,i] = 0\n",
        "\n",
        "      #print results\n",
        "      if y is not None:\n",
        "        print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
        "\n",
        "    else:\n",
        "      # convert probas to one hot vector predictions\n",
        "      prediction = np.argmax(probas, axis=0, out=None)\n",
        "    \n",
        "      for i in range(len(prediction)):\n",
        "          p[prediction[i], i] = 1\n",
        "\n",
        "      #print results\n",
        "      if y is not None:\n",
        "        correct = 0\n",
        "        for i in range(m):\n",
        "          if (p[:, i] == y[:, i]).all():\n",
        "            correct += 1\n",
        "        print(\"Accuracy: \"  + str(correct/m))\n",
        "        \n",
        "    return p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "xkeoJrFZznMf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9849999999999999\n"
          ]
        }
      ],
      "source": [
        "pred_train = predict(X_train, y_train, model, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "mERo3g41zsyX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9999999999999999\n"
          ]
        }
      ],
      "source": [
        "pred_val = predict(X_val, y_val, model, 2)\n",
        "output[\"basic_pred_val\"] = pred_val\n",
        "output[\"basic_layers_dims\"] = layers_dims\n",
        "output[\"basic_activation_fn\"] = activation_fn\n",
        "basic_model_parameters = []\n",
        "for basic_linear in model.linear:\n",
        "  basic_model_parameters.append(basic_linear.parameters)\n",
        "output[\"basic_model_parameters\"] = basic_model_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMCpPFMVdj36"
      },
      "source": [
        "# Advanced implementation (multi class classification)\n",
        "\n",
        "In this section, you need to implement a multi-class classifier using the functions you had previously written. You will create a model that can classify ten handwritten digits. The MNIST handwritten digit classification problem is a standard dataset in computer vision and deep learning. We usually use convolutional deep-learning neural networks for image classification. However, using only dense layers appears to be enough to handle this simple dataset, and this is a good way to get started with image datasets. \n",
        "\n",
        "**Exercise**: Implement a multi-class classifier and tune hyperparameter. (15%)\n",
        "\n",
        "**Instruction**:\n",
        "*   Use the functions you had previously written.\n",
        "*   Preprocess the data to match the correct input format.\n",
        "*   Use mini-batch gradient descent to train the model.\n",
        "\n",
        "**Hint**:\n",
        "For data preprocessing, please be careful with the dimension of the inputs (X and y) and also note that the values of images are usually integers that fall between 0 and 255. You need to change the data type into float and scale the values between 0 and 1.\n",
        "\n",
        "In Batch Gradient Descent, we consider all the samples for every step of Gradient Descent. But what if our dataset is huge? MNIST training data contains 60000 training samples, then to take one step, the model will have to calculate the gradients of all the 60000 samples. This does not seem an efficient way. Hence, mini-batch gradient descent is recommended to be used in this part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "bVSfqnXqXGdC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: X=(60000, 28, 28), y=(60000,)\n",
            "Test: X=(10000, 28, 28)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 9 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of X_train: (784, 60000)\n",
            "shape of y_train: (60000,)\n",
            "shape of X_test: (784, 10000)\n",
            "shape of X_train: (784, 60000)\n",
            "shape of y_train: (60000,)\n",
            "shape of X_test: (784, 10000)\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "data = np.load(\"advanced_data.npz\")\n",
        "X_train = data[\"X_train\"]\n",
        "y_train = data[\"y_train\"].reshape(-1)\n",
        "X_test = data[\"X_test\"]\n",
        "\n",
        "# summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (X_train.shape, y_train.shape))\n",
        "print('Test: X=%s' % (X_test.shape, ))\n",
        "# plot first few images\n",
        "for i in range(9):\n",
        "\t# define subplot\n",
        "\tplt.subplot(330 + 1 + i)\n",
        "\t# plot raw pixel data\n",
        "\tplt.imshow(X_train[i], cmap='gray', vmin=0, vmax=255)\n",
        "# show the figure\n",
        "plt.show()\n",
        "\n",
        "# GRADED CODE: multi-class classification (Data preprocessing)\n",
        "### START CODE HERE ###\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]).T\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2]).T\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(\"shape of X_train: \" + str(X_train.shape))\n",
        "print(\"shape of y_train: \" + str(y_train.shape))\n",
        "print(\"shape of X_test: \" + str(X_test.shape))\n",
        "\n",
        "# GRADED CODE: multi-class classification (Data preprocessing)\n",
        "### START CODE HERE ###\n",
        "\n",
        "\n",
        "X_train = np.divide(X_train, 255)\n",
        "X_test = np.divide(X_test, 255)\n",
        "#y_train = y_train.reshape(y_train.shape[0], 1).T \n",
        "#print(y_train)\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(\"shape of X_train: \" + str(X_train.shape))\n",
        "print(\"shape of y_train: \" + str(y_train.shape))\n",
        "print(\"shape of X_test: \" + str(X_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ljAcf2tpQDR-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6000\n",
            "(784, 4800) (4800,) (784, 1200) (1200,)\n",
            "(784, 48000) (48000,) (784, 12000) (12000,)\n",
            "(784, 48000) (10, 48000) (784, 12000) (10, 12000)\n"
          ]
        }
      ],
      "source": [
        "#You can split training and validation set here. (Optional)\n",
        "### START CODE HERE ###\n",
        "split_value = 0.8\n",
        "slicing = int(X_train.shape[1]/10)\n",
        "print(slicing)\n",
        "x_train_data = X_train[:, : int(slicing*split_value)]\n",
        "y_train_data = y_train[: int(slicing*split_value)]\n",
        "x_vaild_data = X_train[:, int(slicing*split_value): slicing]\n",
        "y_vaild_data = y_train[int(slicing*split_value): slicing]\n",
        "print(x_train_data.shape, y_train_data.shape, x_vaild_data.shape, y_vaild_data.shape)\n",
        "\n",
        "for i in range(1, 10):\n",
        "    x_train_data = np.concatenate([x_train_data, X_train[:, i*slicing : int(i*slicing + slicing*split_value)]], axis=1)\n",
        "    x_vaild_data = np.concatenate([x_vaild_data, X_train[:, int(i*slicing + split_value*slicing) : (i+1)*slicing]], axis=1)\n",
        "    y_train_data = np.concatenate([y_train_data, y_train[i*slicing : int(i*slicing + slicing*split_value)]])\n",
        "    y_vaild_data = np.concatenate([y_vaild_data, y_train[int(i*slicing + split_value*slicing) : (i+1)*slicing]])\n",
        "\n",
        "#y_train_data, y_vaild_data = y_train_data.T, y_vaild_data.T\n",
        "# one hot\n",
        "y_train_data, y_vaild_data = y_train_data.astype(int), y_vaild_data.astype(int)\n",
        "print(x_train_data.shape, y_train_data.shape, x_vaild_data.shape, y_vaild_data.shape)\n",
        "y_train_data, y_vaild_data = np.eye(10)[y_train_data], np.eye(10)[y_vaild_data]      \n",
        "y_train_data, y_vaild_data = y_train_data.T, y_vaild_data.T\n",
        "print(x_train_data.shape, y_train_data.shape, x_vaild_data.shape, y_vaild_data.shape)\n",
        "\n",
        "X_train, y_train, X_val, y_val = x_train_data, y_train_data, x_vaild_data, y_vaild_data\n",
        "\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "HYD-qRs7doU0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.319654\n",
            "Cost after iteration 1: 0.088910\n",
            "Cost after iteration 2: 0.239065\n",
            "Cost after iteration 3: 0.112288\n",
            "Cost after iteration 4: 0.015854\n",
            "Cost after iteration 5: 0.019888\n",
            "Cost after iteration 6: 0.046284\n",
            "Cost after iteration 7: 0.025010\n",
            "Cost after iteration 8: 0.008555\n",
            "Cost after iteration 9: 0.019455\n",
            "Cost after iteration 10: 0.000818\n",
            "Cost after iteration 11: 0.000441\n",
            "Cost after iteration 12: 0.000666\n",
            "Cost after iteration 13: 0.000371\n",
            "Cost after iteration 14: 0.000260\n",
            "Cost after iteration 15: 0.000022\n",
            "Cost after iteration 16: 0.000721\n",
            "Cost after iteration 17: 0.000004\n",
            "Cost after iteration 18: 0.000261\n",
            "Cost after iteration 19: 0.000306\n",
            "Cost after iteration 20: 0.000109\n",
            "Cost after iteration 21: 0.000325\n",
            "Cost after iteration 22: 0.000317\n",
            "Cost after iteration 23: 0.000104\n",
            "Cost after iteration 24: 0.000008\n",
            "Cost after iteration 25: 0.000219\n",
            "Cost after iteration 26: 0.000098\n",
            "Cost after iteration 27: 0.000210\n",
            "Cost after iteration 28: 0.000226\n",
            "Cost after iteration 29: 0.000034\n",
            "Cost after iteration 30: 0.000105\n",
            "Cost after iteration 31: 0.000071\n",
            "Cost after iteration 32: 0.000212\n",
            "Cost after iteration 33: 0.000082\n",
            "Cost after iteration 34: 0.000096\n",
            "Cost after iteration 35: 0.000018\n",
            "Cost after iteration 36: 0.000026\n",
            "Cost after iteration 37: 0.000037\n",
            "Cost after iteration 38: 0.000080\n",
            "Cost after iteration 39: 0.000013\n",
            "Cost after iteration 40: 0.000069\n",
            "Cost after iteration 41: 0.000104\n",
            "Cost after iteration 42: 0.000012\n",
            "Cost after iteration 43: 0.000010\n",
            "Cost after iteration 44: 0.000028\n",
            "Cost after iteration 45: 0.000049\n",
            "Cost after iteration 46: 0.000044\n",
            "Cost after iteration 47: 0.000066\n",
            "Cost after iteration 48: 0.000049\n",
            "Cost after iteration 49: -0.000002\n",
            "Cost after iteration 50: 0.000118\n",
            "Cost after iteration 51: 0.000020\n",
            "Cost after iteration 52: -0.000003\n",
            "Cost after iteration 53: 0.000005\n",
            "Cost after iteration 54: 0.000012\n",
            "Cost after iteration 55: 0.000010\n",
            "Cost after iteration 56: 0.000030\n",
            "Cost after iteration 57: 0.000029\n",
            "Cost after iteration 58: -0.000004\n",
            "Cost after iteration 59: 0.000015\n",
            "Cost after iteration 60: 0.000019\n",
            "Cost after iteration 61: -0.000001\n",
            "Cost after iteration 62: 0.000021\n",
            "Cost after iteration 63: 0.000024\n",
            "Cost after iteration 64: 0.000006\n",
            "Cost after iteration 65: 0.000055\n",
            "Cost after iteration 66: 0.000095\n",
            "Cost after iteration 67: 0.000019\n",
            "Cost after iteration 68: 0.000033\n",
            "Cost after iteration 69: 0.000012\n",
            "Cost after iteration 70: 0.000007\n",
            "Cost after iteration 71: 0.000011\n",
            "Cost after iteration 72: -0.000001\n",
            "Cost after iteration 73: 0.000001\n",
            "Cost after iteration 74: 0.000017\n",
            "Cost after iteration 75: 0.000054\n",
            "Cost after iteration 76: 0.000013\n",
            "Cost after iteration 77: 0.000026\n",
            "Cost after iteration 78: 0.000022\n",
            "Cost after iteration 79: 0.000052\n",
            "Cost after iteration 80: 0.000029\n",
            "Cost after iteration 81: 0.000021\n",
            "Cost after iteration 82: -0.000008\n",
            "Cost after iteration 83: -0.000003\n",
            "Cost after iteration 84: 0.000017\n",
            "Cost after iteration 85: 0.000013\n",
            "Cost after iteration 86: 0.000042\n",
            "Cost after iteration 87: 0.000007\n",
            "Cost after iteration 88: 0.000002\n",
            "Cost after iteration 89: 0.000055\n",
            "Cost after iteration 90: 0.000008\n",
            "Cost after iteration 91: 0.000014\n",
            "Cost after iteration 92: 0.000002\n",
            "Cost after iteration 93: 0.000005\n",
            "Cost after iteration 94: 0.000015\n",
            "Cost after iteration 95: -0.000004\n",
            "Cost after iteration 96: 0.000018\n",
            "Cost after iteration 97: -0.000004\n",
            "Cost after iteration 98: 0.000019\n",
            "Cost after iteration 99: 0.000002\n",
            "Cost after iteration 100: -0.000007\n",
            "Cost after iteration 101: 0.000022\n",
            "Cost after iteration 102: 0.000002\n",
            "Cost after iteration 103: -0.000006\n",
            "Cost after iteration 104: 0.000039\n",
            "Cost after iteration 105: -0.000001\n",
            "Cost after iteration 106: 0.000007\n",
            "Cost after iteration 107: -0.000005\n",
            "Cost after iteration 108: -0.000007\n",
            "Cost after iteration 109: 0.000033\n",
            "Cost after iteration 110: 0.000003\n",
            "Cost after iteration 111: 0.000020\n",
            "Cost after iteration 112: 0.000030\n",
            "Cost after iteration 113: -0.000007\n",
            "Cost after iteration 114: 0.000011\n",
            "Cost after iteration 115: 0.000021\n",
            "Cost after iteration 116: 0.000007\n",
            "Cost after iteration 117: -0.000000\n",
            "Cost after iteration 118: -0.000007\n",
            "Cost after iteration 119: 0.000001\n",
            "Cost after iteration 120: -0.000001\n",
            "Cost after iteration 121: 0.000015\n",
            "Cost after iteration 122: -0.000008\n",
            "Cost after iteration 123: 0.000013\n",
            "Cost after iteration 124: 0.000006\n",
            "Cost after iteration 125: 0.000016\n",
            "Cost after iteration 126: -0.000004\n",
            "Cost after iteration 127: -0.000000\n",
            "Cost after iteration 128: 0.000004\n",
            "Cost after iteration 129: 0.000003\n",
            "Cost after iteration 130: -0.000005\n",
            "Cost after iteration 131: 0.000003\n",
            "Cost after iteration 132: 0.000008\n",
            "Cost after iteration 133: 0.000009\n",
            "Cost after iteration 134: -0.000004\n",
            "Cost after iteration 135: -0.000008\n",
            "Cost after iteration 136: -0.000005\n",
            "Cost after iteration 137: 0.000012\n",
            "Cost after iteration 138: 0.000005\n",
            "Cost after iteration 139: 0.000011\n",
            "Cost after iteration 140: 0.000009\n",
            "Cost after iteration 141: 0.000004\n",
            "Cost after iteration 142: 0.000005\n",
            "Cost after iteration 143: -0.000003\n",
            "Cost after iteration 144: 0.000005\n",
            "Cost after iteration 145: 0.000004\n",
            "Cost after iteration 146: 0.000000\n",
            "Cost after iteration 147: 0.000003\n",
            "Cost after iteration 148: -0.000000\n",
            "Cost after iteration 149: 0.000000\n",
            "Cost after iteration 150: 0.000005\n",
            "Cost after iteration 151: 0.000006\n",
            "Cost after iteration 152: 0.000014\n",
            "Cost after iteration 153: 0.000001\n",
            "Cost after iteration 154: 0.000007\n",
            "Cost after iteration 155: 0.000015\n",
            "Cost after iteration 156: 0.000008\n",
            "Cost after iteration 157: 0.000008\n",
            "Cost after iteration 158: 0.000012\n",
            "Cost after iteration 159: -0.000006\n",
            "Cost after iteration 160: 0.000004\n",
            "Cost after iteration 161: 0.000014\n",
            "Cost after iteration 162: 0.000004\n",
            "Cost after iteration 163: -0.000009\n",
            "Cost after iteration 164: 0.000003\n",
            "Cost after iteration 165: 0.000004\n",
            "Cost after iteration 166: 0.000007\n",
            "Cost after iteration 167: -0.000006\n",
            "Cost after iteration 168: 0.000001\n",
            "Cost after iteration 169: 0.000005\n",
            "Cost after iteration 170: 0.000004\n",
            "Cost after iteration 171: -0.000001\n",
            "Cost after iteration 172: 0.000015\n",
            "Cost after iteration 173: 0.000006\n",
            "Cost after iteration 174: -0.000004\n",
            "Cost after iteration 175: 0.000012\n",
            "Cost after iteration 176: -0.000002\n",
            "Cost after iteration 177: 0.000001\n",
            "Cost after iteration 178: 0.000023\n",
            "Cost after iteration 179: -0.000006\n",
            "Cost after iteration 180: -0.000009\n",
            "Cost after iteration 181: -0.000009\n",
            "Cost after iteration 182: 0.000001\n",
            "Cost after iteration 183: -0.000001\n",
            "Cost after iteration 184: 0.000010\n",
            "Cost after iteration 185: -0.000002\n",
            "Cost after iteration 186: -0.000007\n",
            "Cost after iteration 187: -0.000003\n",
            "Cost after iteration 188: 0.000008\n",
            "Cost after iteration 189: 0.000011\n",
            "Cost after iteration 190: 0.000006\n",
            "Cost after iteration 191: 0.000007\n",
            "Cost after iteration 192: -0.000007\n",
            "Cost after iteration 193: -0.000008\n",
            "Cost after iteration 194: -0.000005\n",
            "Cost after iteration 195: -0.000001\n",
            "Cost after iteration 196: 0.000001\n",
            "Cost after iteration 197: -0.000005\n",
            "Cost after iteration 198: -0.000003\n",
            "Cost after iteration 199: 0.000012\n",
            "Cost after iteration 200: -0.000008\n",
            "Cost after iteration 201: -0.000006\n",
            "Cost after iteration 202: -0.000009\n",
            "Cost after iteration 203: 0.000000\n",
            "Cost after iteration 204: -0.000005\n",
            "Cost after iteration 205: -0.000002\n",
            "Cost after iteration 206: -0.000002\n",
            "Cost after iteration 207: -0.000004\n",
            "Cost after iteration 208: 0.000006\n",
            "Cost after iteration 209: -0.000008\n",
            "Cost after iteration 210: -0.000002\n",
            "Cost after iteration 211: -0.000002\n",
            "Cost after iteration 212: -0.000002\n",
            "Cost after iteration 213: -0.000007\n",
            "Cost after iteration 214: -0.000007\n",
            "Cost after iteration 215: -0.000007\n",
            "Cost after iteration 216: -0.000004\n",
            "Cost after iteration 217: -0.000002\n",
            "Cost after iteration 218: -0.000001\n",
            "Cost after iteration 219: 0.000000\n",
            "Cost after iteration 220: -0.000002\n",
            "Cost after iteration 221: -0.000005\n",
            "Cost after iteration 222: -0.000006\n",
            "Cost after iteration 223: -0.000005\n",
            "Cost after iteration 224: -0.000005\n",
            "Cost after iteration 225: -0.000009\n",
            "Cost after iteration 226: 0.000003\n",
            "Cost after iteration 227: -0.000002\n",
            "Cost after iteration 228: 0.000000\n",
            "Cost after iteration 229: -0.000009\n",
            "Cost after iteration 230: -0.000004\n",
            "Cost after iteration 231: -0.000001\n",
            "Cost after iteration 232: -0.000009\n",
            "Cost after iteration 233: -0.000006\n",
            "Cost after iteration 234: 0.000007\n",
            "Cost after iteration 235: -0.000004\n",
            "Cost after iteration 236: -0.000005\n",
            "Cost after iteration 237: -0.000001\n",
            "Cost after iteration 238: -0.000002\n",
            "Cost after iteration 239: -0.000006\n",
            "Cost after iteration 240: -0.000002\n",
            "Cost after iteration 241: 0.000002\n",
            "Cost after iteration 242: -0.000005\n",
            "Cost after iteration 243: -0.000005\n",
            "Cost after iteration 244: -0.000002\n",
            "Cost after iteration 245: -0.000008\n",
            "Cost after iteration 246: -0.000008\n",
            "Cost after iteration 247: -0.000004\n",
            "Cost after iteration 248: -0.000002\n",
            "Cost after iteration 249: -0.000009\n",
            "Cost after iteration 250: -0.000004\n",
            "Cost after iteration 251: -0.000004\n",
            "Cost after iteration 252: -0.000009\n",
            "Cost after iteration 253: -0.000007\n",
            "Cost after iteration 254: 0.000001\n",
            "Cost after iteration 255: -0.000002\n",
            "Cost after iteration 256: 0.000004\n",
            "Cost after iteration 257: -0.000001\n",
            "Cost after iteration 258: -0.000004\n",
            "Cost after iteration 259: -0.000005\n",
            "Cost after iteration 260: -0.000009\n",
            "Cost after iteration 261: -0.000006\n",
            "Cost after iteration 262: -0.000001\n",
            "Cost after iteration 263: -0.000009\n",
            "Cost after iteration 264: -0.000006\n",
            "Cost after iteration 265: -0.000004\n",
            "Cost after iteration 266: 0.000000\n",
            "Cost after iteration 267: 0.000002\n",
            "Cost after iteration 268: 0.000002\n",
            "Cost after iteration 269: -0.000000\n",
            "Cost after iteration 270: -0.000008\n",
            "Cost after iteration 271: -0.000004\n",
            "Cost after iteration 272: -0.000008\n",
            "Cost after iteration 273: -0.000004\n",
            "Cost after iteration 274: -0.000001\n",
            "Cost after iteration 275: 0.000001\n",
            "Cost after iteration 276: -0.000006\n",
            "Cost after iteration 277: -0.000008\n",
            "Cost after iteration 278: -0.000007\n",
            "Cost after iteration 279: -0.000003\n",
            "Cost after iteration 280: -0.000009\n",
            "Cost after iteration 281: -0.000001\n",
            "Cost after iteration 282: 0.000000\n",
            "Cost after iteration 283: -0.000006\n",
            "Cost after iteration 284: -0.000005\n",
            "Cost after iteration 285: 0.000001\n",
            "Cost after iteration 286: -0.000002\n",
            "Cost after iteration 287: -0.000007\n",
            "Cost after iteration 288: -0.000006\n",
            "Cost after iteration 289: -0.000006\n",
            "Cost after iteration 290: -0.000008\n",
            "Cost after iteration 291: -0.000005\n",
            "Cost after iteration 292: 0.000003\n",
            "Cost after iteration 293: -0.000008\n",
            "Cost after iteration 294: -0.000004\n",
            "Cost after iteration 295: 0.000008\n",
            "Cost after iteration 296: -0.000008\n",
            "Cost after iteration 297: -0.000004\n",
            "Cost after iteration 298: -0.000002\n",
            "Cost after iteration 299: -0.000008\n",
            "Cost after iteration 300: -0.000003\n",
            "Cost after iteration 301: -0.000008\n",
            "Cost after iteration 302: -0.000008\n",
            "Cost after iteration 303: -0.000007\n",
            "Cost after iteration 304: -0.000000\n",
            "Cost after iteration 305: -0.000007\n",
            "Cost after iteration 306: -0.000004\n",
            "Cost after iteration 307: -0.000008\n",
            "Cost after iteration 308: -0.000006\n",
            "Cost after iteration 309: -0.000004\n",
            "Cost after iteration 310: -0.000009\n",
            "Cost after iteration 311: 0.000001\n",
            "Cost after iteration 312: -0.000002\n",
            "Cost after iteration 313: -0.000005\n",
            "Cost after iteration 314: -0.000010\n",
            "Cost after iteration 315: -0.000006\n",
            "Cost after iteration 316: 0.000001\n",
            "Cost after iteration 317: -0.000009\n",
            "Cost after iteration 318: -0.000003\n",
            "Cost after iteration 319: -0.000002\n",
            "Cost after iteration 320: -0.000008\n",
            "Cost after iteration 321: -0.000009\n",
            "Cost after iteration 322: -0.000005\n",
            "Cost after iteration 323: -0.000003\n",
            "Cost after iteration 324: -0.000008\n",
            "Cost after iteration 325: -0.000003\n",
            "Cost after iteration 326: -0.000009\n",
            "Cost after iteration 327: -0.000008\n",
            "Cost after iteration 328: -0.000004\n",
            "Cost after iteration 329: -0.000009\n",
            "Cost after iteration 330: -0.000001\n",
            "Cost after iteration 331: -0.000001\n",
            "Cost after iteration 332: -0.000009\n",
            "Cost after iteration 333: -0.000000\n",
            "Cost after iteration 334: -0.000005\n",
            "Cost after iteration 335: -0.000002\n",
            "Cost after iteration 336: -0.000008\n",
            "Cost after iteration 337: -0.000004\n",
            "Cost after iteration 338: -0.000009\n",
            "Cost after iteration 339: -0.000003\n",
            "Cost after iteration 340: -0.000000\n",
            "Cost after iteration 341: -0.000003\n",
            "Cost after iteration 342: -0.000007\n",
            "Cost after iteration 343: -0.000007\n",
            "Cost after iteration 344: -0.000006\n",
            "Cost after iteration 345: -0.000003\n",
            "Cost after iteration 346: -0.000005\n",
            "Cost after iteration 347: -0.000009\n",
            "Cost after iteration 348: -0.000003\n",
            "Cost after iteration 349: 0.000000\n",
            "Cost after iteration 350: -0.000008\n",
            "Cost after iteration 351: -0.000007\n",
            "Cost after iteration 352: -0.000007\n",
            "Cost after iteration 353: -0.000003\n",
            "Cost after iteration 354: -0.000008\n",
            "Cost after iteration 355: -0.000006\n",
            "Cost after iteration 356: -0.000009\n",
            "Cost after iteration 357: -0.000007\n",
            "Cost after iteration 358: -0.000005\n",
            "Cost after iteration 359: -0.000009\n",
            "Cost after iteration 360: -0.000006\n",
            "Cost after iteration 361: -0.000006\n",
            "Cost after iteration 362: -0.000006\n",
            "Cost after iteration 363: -0.000005\n",
            "Cost after iteration 364: -0.000007\n",
            "Cost after iteration 365: -0.000003\n",
            "Cost after iteration 366: -0.000006\n",
            "Cost after iteration 367: -0.000008\n",
            "Cost after iteration 368: -0.000003\n",
            "Cost after iteration 369: -0.000007\n",
            "Cost after iteration 370: -0.000009\n",
            "Cost after iteration 371: -0.000005\n",
            "Cost after iteration 372: -0.000008\n",
            "Cost after iteration 373: -0.000009\n",
            "Cost after iteration 374: -0.000008\n",
            "Cost after iteration 375: -0.000009\n",
            "Cost after iteration 376: -0.000005\n",
            "Cost after iteration 377: -0.000007\n",
            "Cost after iteration 378: -0.000009\n",
            "Cost after iteration 379: -0.000007\n",
            "Cost after iteration 380: -0.000008\n",
            "Cost after iteration 381: -0.000003\n",
            "Cost after iteration 382: -0.000008\n",
            "Cost after iteration 383: -0.000008\n",
            "Cost after iteration 384: -0.000009\n",
            "Cost after iteration 385: -0.000007\n",
            "Cost after iteration 386: -0.000003\n",
            "Cost after iteration 387: -0.000000\n",
            "Cost after iteration 388: -0.000009\n",
            "Cost after iteration 389: -0.000006\n",
            "Cost after iteration 390: -0.000009\n",
            "Cost after iteration 391: -0.000001\n",
            "Cost after iteration 392: -0.000008\n",
            "Cost after iteration 393: -0.000007\n",
            "Cost after iteration 394: -0.000009\n",
            "Cost after iteration 395: -0.000005\n",
            "Cost after iteration 396: -0.000007\n",
            "Cost after iteration 397: -0.000005\n",
            "Cost after iteration 398: -0.000010\n",
            "Cost after iteration 399: -0.000002\n",
            "Cost after iteration 400: -0.000008\n",
            "Cost after iteration 401: -0.000004\n",
            "Cost after iteration 402: -0.000007\n",
            "Cost after iteration 403: -0.000004\n",
            "Cost after iteration 404: -0.000005\n",
            "Cost after iteration 405: -0.000010\n",
            "Cost after iteration 406: -0.000003\n",
            "Cost after iteration 407: -0.000005\n",
            "Cost after iteration 408: -0.000005\n",
            "Cost after iteration 409: -0.000010\n",
            "Cost after iteration 410: -0.000006\n",
            "Cost after iteration 411: -0.000007\n",
            "Cost after iteration 412: -0.000008\n",
            "Cost after iteration 413: -0.000008\n",
            "Cost after iteration 414: -0.000006\n",
            "Cost after iteration 415: -0.000008\n",
            "Cost after iteration 416: -0.000007\n",
            "Cost after iteration 417: -0.000002\n",
            "Cost after iteration 418: -0.000005\n",
            "Cost after iteration 419: -0.000008\n",
            "Cost after iteration 420: -0.000003\n",
            "Cost after iteration 421: 0.000003\n",
            "Cost after iteration 422: -0.000009\n",
            "Cost after iteration 423: -0.000006\n",
            "Cost after iteration 424: 0.000000\n",
            "Cost after iteration 425: -0.000004\n",
            "Cost after iteration 426: -0.000008\n",
            "Cost after iteration 427: -0.000005\n",
            "Cost after iteration 428: -0.000007\n",
            "Cost after iteration 429: -0.000006\n",
            "Cost after iteration 430: -0.000002\n",
            "Cost after iteration 431: -0.000006\n",
            "Cost after iteration 432: -0.000004\n",
            "Cost after iteration 433: -0.000008\n",
            "Cost after iteration 434: -0.000009\n",
            "Cost after iteration 435: -0.000004\n",
            "Cost after iteration 436: -0.000007\n",
            "Cost after iteration 437: -0.000009\n",
            "Cost after iteration 438: -0.000009\n",
            "Cost after iteration 439: -0.000001\n",
            "Cost after iteration 440: -0.000004\n",
            "Cost after iteration 441: -0.000004\n",
            "Cost after iteration 442: -0.000009\n",
            "Cost after iteration 443: -0.000009\n",
            "Cost after iteration 444: -0.000007\n",
            "Cost after iteration 445: -0.000005\n",
            "Cost after iteration 446: -0.000003\n",
            "Cost after iteration 447: -0.000006\n",
            "Cost after iteration 448: -0.000002\n",
            "Cost after iteration 449: -0.000004\n",
            "Cost after iteration 450: -0.000003\n",
            "Cost after iteration 451: -0.000007\n",
            "Cost after iteration 452: -0.000006\n",
            "Cost after iteration 453: -0.000008\n",
            "Cost after iteration 454: -0.000002\n",
            "Cost after iteration 455: -0.000010\n",
            "Cost after iteration 456: -0.000008\n",
            "Cost after iteration 457: -0.000006\n",
            "Cost after iteration 458: -0.000005\n",
            "Cost after iteration 459: -0.000008\n",
            "Cost after iteration 460: -0.000006\n",
            "Cost after iteration 461: -0.000004\n",
            "Cost after iteration 462: -0.000010\n",
            "Cost after iteration 463: -0.000008\n",
            "Cost after iteration 464: -0.000009\n",
            "Cost after iteration 465: -0.000009\n",
            "Cost after iteration 466: -0.000006\n",
            "Cost after iteration 467: -0.000008\n",
            "Cost after iteration 468: -0.000007\n",
            "Cost after iteration 469: -0.000008\n",
            "Cost after iteration 470: -0.000006\n",
            "Cost after iteration 471: -0.000007\n",
            "Cost after iteration 472: -0.000007\n",
            "Cost after iteration 473: -0.000010\n",
            "Cost after iteration 474: -0.000009\n",
            "Cost after iteration 475: -0.000007\n",
            "Cost after iteration 476: -0.000007\n",
            "Cost after iteration 477: -0.000008\n",
            "Cost after iteration 478: -0.000005\n",
            "Cost after iteration 479: -0.000005\n",
            "Cost after iteration 480: -0.000006\n",
            "Cost after iteration 481: -0.000006\n",
            "Cost after iteration 482: -0.000008\n",
            "Cost after iteration 483: -0.000007\n",
            "Cost after iteration 484: -0.000008\n",
            "Cost after iteration 485: -0.000005\n",
            "Cost after iteration 486: -0.000008\n",
            "Cost after iteration 487: -0.000008\n",
            "Cost after iteration 488: -0.000008\n",
            "Cost after iteration 489: -0.000004\n",
            "Cost after iteration 490: -0.000003\n",
            "Cost after iteration 491: -0.000006\n",
            "Cost after iteration 492: -0.000008\n",
            "Cost after iteration 493: -0.000007\n",
            "Cost after iteration 494: -0.000008\n",
            "Cost after iteration 495: -0.000009\n",
            "Cost after iteration 496: -0.000005\n",
            "Cost after iteration 497: -0.000007\n",
            "Cost after iteration 498: -0.000009\n",
            "Cost after iteration 499: -0.000008\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# GRADED CODE: multi-class classification\n",
        "### START CODE HERE ###\n",
        "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (input size, number of examples)\n",
        "    Y -- true \"label\" vector, of shape (number of classes, number of examples)\n",
        "    mini_batch_size -- size of the mini-batches, integer\n",
        "    \n",
        "    Returns:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]                  # number of training examples\n",
        "    mini_batches = []\n",
        "        \n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation]\n",
        "    \n",
        "    inc = mini_batch_size\n",
        "\n",
        "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
        "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
        "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        # (approx. 2 lines)\n",
        "        mini_batch_X = shuffled_X[:, k*inc: (k+1)*inc]\n",
        "        mini_batch_Y = shuffled_Y[:, k*inc: (k+1)*inc]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
        "    if m % mini_batch_size != 0:\n",
        "        #(approx. 2 lines)\n",
        "        mini_batch_X = shuffled_X[:, (num_complete_minibatches+1)*inc: ]\n",
        "        mini_batch_Y = shuffled_Y[:, (num_complete_minibatches+1)*inc: ]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    return mini_batches\n",
        "\n",
        "layers_dims = [784, 888, 444, 222, 111, 56, 10]\n",
        "activation_fn = [\"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"softmax\"]\n",
        "learning_rate = 0.062211\n",
        "num_iterations = 500\n",
        "batch_size = 64\n",
        "print_cost = True\n",
        "classes = 10\n",
        "costs = []                         # keep track of cost\n",
        "model = Model(layers_dims, activation_fn)\n",
        "\n",
        "# Loop (gradient descent)\n",
        "for i in range(0, num_iterations):\n",
        "    mini_batches = random_mini_batches(X_train, y_train, batch_size)\n",
        "    for batch in mini_batches:\n",
        "        x_batch, y_batch = batch\n",
        "        \n",
        "        # forward\n",
        "        #x_batch = x_batch.T\n",
        "        AL = model.forward(x_batch)\n",
        "\n",
        "        # compute cost\n",
        "        if classes == 2:\n",
        "            cost = compute_BCE_cost(AL, y_batch)\n",
        "        else:\n",
        "            cost = compute_CCE_cost(AL, y_batch)\n",
        "\n",
        "        # backward\n",
        "        dA_prev = model.backward(AL, y_batch)\n",
        "        # update\n",
        "        model.update(learning_rate)\n",
        "\n",
        "    if print_cost and i % 1 == 0:\n",
        "        print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        costs.append(cost)\n",
        "\n",
        "# plot the cost\n",
        "plt.plot(np.squeeze(costs))\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "plt.show()\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "yI92fh4JXC1k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "pred_train = predict(X_train, y_train, model, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ehjcfSU2XD3-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9803333333333333\n"
          ]
        }
      ],
      "source": [
        "#You can check for your validation accuracy here. (Optional)\n",
        "### START CODE HERE ###\n",
        "pred_val = predict(X_val, y_val, model, 10)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "YHFDuq2BQ2qI"
      },
      "outputs": [],
      "source": [
        "pred_test = predict(X_test, None, model, 10)\n",
        "output[\"advanced_pred_test\"] = pred_test\n",
        "output[\"advanced_layers_dims\"] = layers_dims\n",
        "output[\"advanced_activation_fn\"] = activation_fn\n",
        "advanced_model_parameters = []\n",
        "for advanced_linear in model.linear:\n",
        "  advanced_model_parameters.append(advanced_linear.parameters)\n",
        "output[\"advanced_model_parameters\"] = advanced_model_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXGnS3HQeNUc"
      },
      "source": [
        "# Submit prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "twMsmXbQeDL_"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "assert(list(output.keys()) == ['linear_initialize_parameters', 'linear_forward', 'linear_backward', 'linear_update_parameters', 'sigmoid', 'relu', 'softmax', 'sigmoid_backward', 'relu_backward', 'softmax_CCE_backward', 'model_initialize_parameters', 'model_forward_sigmoid', 'model_forward_relu', 'model_forward_softmax', 'model_backward_sigmoid', 'model_backward_relu', 'model_update_parameters', 'compute_BCE_cost', 'compute_CCE_cost', 'basic_pred_val', 'basic_layers_dims', 'basic_activation_fn', 'basic_model_parameters', 'advanced_pred_test', 'advanced_layers_dims', 'advanced_activation_fn', 'advanced_model_parameters'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "bCJ0XTO_zE8A"
      },
      "outputs": [],
      "source": [
        "np.save(\"output.npy\", output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "wFBFUUEg1to-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "linear_initialize_parameters： <class 'dict'>\n",
            "linear_forward： <class 'tuple'>\n",
            "linear_backward： <class 'tuple'>\n",
            "linear_update_parameters： <class 'dict'>\n",
            "sigmoid： <class 'tuple'>\n",
            "relu： <class 'tuple'>\n",
            "softmax： <class 'tuple'>\n",
            "sigmoid_backward： <class 'numpy.ndarray'>\n",
            "relu_backward： <class 'numpy.ndarray'>\n",
            "softmax_CCE_backward： <class 'numpy.ndarray'>\n",
            "model_initialize_parameters： <class 'tuple'>\n",
            "model_forward_sigmoid： <class 'tuple'>\n",
            "model_forward_relu： <class 'tuple'>\n",
            "model_forward_softmax： <class 'tuple'>\n",
            "model_backward_sigmoid： <class 'tuple'>\n",
            "model_backward_relu： <class 'tuple'>\n",
            "model_update_parameters： <class 'dict'>\n",
            "compute_BCE_cost： <class 'numpy.float64'>\n",
            "compute_CCE_cost： <class 'numpy.float64'>\n",
            "basic_pred_val： <class 'numpy.ndarray'>\n",
            "basic_layers_dims： <class 'list'>\n",
            "basic_activation_fn： <class 'list'>\n",
            "basic_model_parameters： <class 'list'>\n",
            "advanced_pred_test： <class 'numpy.ndarray'>\n",
            "advanced_layers_dims： <class 'list'>\n",
            "advanced_activation_fn： <class 'list'>\n",
            "advanced_model_parameters： <class 'list'>\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "submit = np.load(\"output.npy\", allow_pickle=True).item()\n",
        "for key, value in submit.items():\n",
        "  print(str(key) + \"： \" + str(type(value)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trQqZni7jhP0"
      },
      "source": [
        "Expected output: <br>\n",
        "<small>\n",
        "linear_initialize_parameters： <class 'dict'> <br>\n",
        "linear_forward： <class 'tuple'> <br>\n",
        "linear_backward： <class 'tuple'> <br>\n",
        "linear_update_parameters： <class 'dict'> <br>\n",
        "sigmoid： <class 'tuple'> <br>\n",
        "relu： <class 'tuple'> <br>\n",
        "softmax： <class 'tuple'> <br>\n",
        "sigmoid_backward： <class 'numpy.ndarray'> <br>\n",
        "relu_backward： <class 'numpy.ndarray'> <br>\n",
        "softmax_CCE_backward： <class 'numpy.ndarray'> <br>\n",
        "model_initialize_parameters： <class 'tuple'> <br>\n",
        "model_forward_sigmoid： <class 'tuple'> <br>\n",
        "model_forward_relu： <class 'tuple'> <br>\n",
        "model_forward_softmax： <class 'tuple'> <br>\n",
        "model_backward_sigmoid： <class 'tuple'> <br>\n",
        "model_backward_relu： <class 'tuple'> <br>\n",
        "model_update_parameters： <class 'dict'> <br>\n",
        "compute_BCE_cost： <class 'numpy.ndarray'> <br> \n",
        "compute_CCE_cost： <class 'numpy.ndarray'> <br>\n",
        "basic_pred_val： <class 'numpy.ndarray'> <br>\n",
        "basic_layers_dims： <class 'list'> <br>\n",
        "basic_activation_fn： <class 'list'> <br>\n",
        "basic_model_parameters： <class 'list'> <br>\n",
        "advanced_pred_test： <class 'numpy.ndarray'> <br>\n",
        "advanced_layers_dims： <class 'list'> <br>\n",
        "advanced_activation_fn： <class 'list'> <br>\n",
        "advanced_model_parameters： <class 'list'> <br>\n",
        "</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GRvMDwalE5y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
